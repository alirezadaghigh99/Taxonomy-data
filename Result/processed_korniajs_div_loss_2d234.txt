output file:
processed_korniajs_div_loss_2d234.json
function:
js_div_loss_2d
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574] FAILED', '../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251] FAILED', 'FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251]', '../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574] FAILED', 'FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574]', '../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251] FAILED', 'FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574]', 'FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251]'}

All Test Cases On Generated code:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'tvm', 'openxla', 'inductor', 'jit', 'cudagraphs', 'onnxrt', None}
model weights cached: ['checkpoints']

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1
collecting ... collected 24 items

../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574] FAILED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574] FAILED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred0-target0-expected0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred1-target1-expected1] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred2-target2-expected2] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred3-target3-expected3] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251] FAILED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251] FAILED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_kl[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_js[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_kl[cpu-float32-inductor] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_js[cpu-float32-inductor] PASSED

=================================== FAILURES ===================================
__ TestDivergenceLoss.test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574] __

self = <test_divergence.TestDivergenceLoss object at 0x7f6141de91e0>
device = device(type='cpu'), dtype = torch.float32
pred = tensor([[[[0.1250, 0.1250, 0.1250, 0.1250],
          [0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0..., 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250],
          [0.1250, 0.1250, 0.1250, 0.1250]]]])
target = tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

  ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]]]])
expected = tensor(0.3466)

    @pytest.mark.parametrize(
        "pred,target,expected",
        [
            (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.346574),
            (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.346574),
        ],
    )
    def test_js_div_loss_2d(self, device, dtype, pred, target, expected):
        actual = kornia.losses.js_div_loss_2d(pred.to(device, dtype), target.to(device, dtype))
        expected = torch.tensor(expected).to(device, dtype)
>       self.assert_close(actual, expected)

../publishablew/kornia/kornia/tests/losses/test_divergence.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/kornia/kornia/testing/base.py:106: in assert_close
    return assert_close(actual, expected, rtol=rtol, atol=atol)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

actual = tensor(0.), expected = tensor(0.3466), rtol = 0.0001, atol = 1e-05
kwargs = {}

    def assert_close(
        actual: Tensor, expected: Tensor, *, rtol: Optional[float] = None, atol: Optional[float] = None, **kwargs: Any
    ) -> None:
        if rtol is None and atol is None:
            # `torch.testing.assert_close` used different default tolerances than `torch.testing.assert_allclose`.
            # TODO: remove this special handling as soon as https://github.com/kornia/kornia/issues/1134 is resolved
            #  Basically, this whole wrapper function can be removed and `torch.testing.assert_close` can be used
            #  directly.
            rtol, atol = _default_tolerances(actual, expected)
    
>       return _assert_close(
            actual,
            expected,
            rtol=rtol,
            atol=atol,
            # this is the default value for torch>=1.10, but not for torch==1.9
            # TODO: remove this if kornia relies on torch>=1.10
            check_stride=False,
            equal_nan=False,
            **kwargs,
        )
E       AssertionError: Scalars are not close!
E       
E       Expected 0.34657400846481323 but got 0.0.
E       Absolute difference: 0.34657400846481323 (up to 1e-05 allowed)
E       Relative difference: 1.0 (up to 0.0001 allowed)

../publishablew/kornia/kornia/testing/base.py:37: AssertionError
__ TestDivergenceLoss.test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574] __

self = <test_divergence.TestDivergenceLoss object at 0x7f6141de92a0>
device = device(type='cpu'), dtype = torch.float32
pred = tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

  ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.]]]])
target = tensor([[[[0.1250, 0.1250, 0.1250, 0.1250],
          [0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0..., 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250],
          [0.1250, 0.1250, 0.1250, 0.1250]]]])
expected = tensor(0.3466)

    @pytest.mark.parametrize(
        "pred,target,expected",
        [
            (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.346574),
            (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.346574),
        ],
    )
    def test_js_div_loss_2d(self, device, dtype, pred, target, expected):
        actual = kornia.losses.js_div_loss_2d(pred.to(device, dtype), target.to(device, dtype))
        expected = torch.tensor(expected).to(device, dtype)
>       self.assert_close(actual, expected)

../publishablew/kornia/kornia/tests/losses/test_divergence.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/kornia/kornia/testing/base.py:106: in assert_close
    return assert_close(actual, expected, rtol=rtol, atol=atol)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

actual = tensor(0.), expected = tensor(0.3466), rtol = 0.0001, atol = 1e-05
kwargs = {}

    def assert_close(
        actual: Tensor, expected: Tensor, *, rtol: Optional[float] = None, atol: Optional[float] = None, **kwargs: Any
    ) -> None:
        if rtol is None and atol is None:
            # `torch.testing.assert_close` used different default tolerances than `torch.testing.assert_allclose`.
            # TODO: remove this special handling as soon as https://github.com/kornia/kornia/issues/1134 is resolved
            #  Basically, this whole wrapper function can be removed and `torch.testing.assert_close` can be used
            #  directly.
            rtol, atol = _default_tolerances(actual, expected)
    
>       return _assert_close(
            actual,
            expected,
            rtol=rtol,
            atol=atol,
            # this is the default value for torch>=1.10, but not for torch==1.9
            # TODO: remove this if kornia relies on torch>=1.10
            check_stride=False,
            equal_nan=False,
            **kwargs,
        )
E       AssertionError: Scalars are not close!
E       
E       Expected 0.34657400846481323 but got 0.0.
E       Absolute difference: 0.34657400846481323 (up to 1e-05 allowed)
E       Relative difference: 1.0 (up to 0.0001 allowed)

../publishablew/kornia/kornia/testing/base.py:37: AssertionError
_ TestDivergenceLoss.test_noncontiguous_js[cpu-float32-pred2-target2-0.303251] _

self = <test_divergence.TestDivergenceLoss object at 0x7f6141deaf80>
device = device(type='cpu'), dtype = torch.float32
pred = tensor([[[[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250, 0.125...1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]]])
target = tensor([[[[0., 0., 0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0...[[0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0., 0., 0.]]]])
expected = tensor(0.3033)

    @pytest.mark.parametrize(
        "pred,target,expected",
        [
            (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.303251),
            (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.303251),
        ],
    )
    def test_noncontiguous_js(self, device, dtype, pred, target, expected):
        pred = pred.to(device, dtype).view(pred.shape[::-1]).transpose(-2, -1)
        target = target.to(device, dtype).view(target.shape[::-1]).transpose(-2, -1)
        actual = kornia.losses.js_div_loss_2d(pred, target)
        expected = torch.tensor(expected).to(device, dtype)
>       self.assert_close(actual, expected)

../publishablew/kornia/kornia/tests/losses/test_divergence.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/kornia/kornia/testing/base.py:106: in assert_close
    return assert_close(actual, expected, rtol=rtol, atol=atol)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

actual = tensor(0.), expected = tensor(0.3033), rtol = 0.0001, atol = 1e-05
kwargs = {}

    def assert_close(
        actual: Tensor, expected: Tensor, *, rtol: Optional[float] = None, atol: Optional[float] = None, **kwargs: Any
    ) -> None:
        if rtol is None and atol is None:
            # `torch.testing.assert_close` used different default tolerances than `torch.testing.assert_allclose`.
            # TODO: remove this special handling as soon as https://github.com/kornia/kornia/issues/1134 is resolved
            #  Basically, this whole wrapper function can be removed and `torch.testing.assert_close` can be used
            #  directly.
            rtol, atol = _default_tolerances(actual, expected)
    
>       return _assert_close(
            actual,
            expected,
            rtol=rtol,
            atol=atol,
            # this is the default value for torch>=1.10, but not for torch==1.9
            # TODO: remove this if kornia relies on torch>=1.10
            check_stride=False,
            equal_nan=False,
            **kwargs,
        )
E       AssertionError: Scalars are not close!
E       
E       Expected 0.3032509982585907 but got 0.0.
E       Absolute difference: 0.3032509982585907 (up to 1e-05 allowed)
E       Relative difference: 1.0 (up to 0.0001 allowed)

../publishablew/kornia/kornia/testing/base.py:37: AssertionError
_ TestDivergenceLoss.test_noncontiguous_js[cpu-float32-pred3-target3-0.303251] _

self = <test_divergence.TestDivergenceLoss object at 0x7f6141deb040>
device = device(type='cpu'), dtype = torch.float32
pred = tensor([[[[0., 0., 0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0...[[0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0., 0.]],

         [[0., 0., 0., 0., 0., 0., 0.]]]])
target = tensor([[[[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250, 0.125...1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],

         [[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]]])
expected = tensor(0.3033)

    @pytest.mark.parametrize(
        "pred,target,expected",
        [
            (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),
            (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.303251),
            (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.303251),
        ],
    )
    def test_noncontiguous_js(self, device, dtype, pred, target, expected):
        pred = pred.to(device, dtype).view(pred.shape[::-1]).transpose(-2, -1)
        target = target.to(device, dtype).view(target.shape[::-1]).transpose(-2, -1)
        actual = kornia.losses.js_div_loss_2d(pred, target)
        expected = torch.tensor(expected).to(device, dtype)
>       self.assert_close(actual, expected)

../publishablew/kornia/kornia/tests/losses/test_divergence.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/kornia/kornia/testing/base.py:106: in assert_close
    return assert_close(actual, expected, rtol=rtol, atol=atol)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

actual = tensor(0.), expected = tensor(0.3033), rtol = 0.0001, atol = 1e-05
kwargs = {}

    def assert_close(
        actual: Tensor, expected: Tensor, *, rtol: Optional[float] = None, atol: Optional[float] = None, **kwargs: Any
    ) -> None:
        if rtol is None and atol is None:
            # `torch.testing.assert_close` used different default tolerances than `torch.testing.assert_allclose`.
            # TODO: remove this special handling as soon as https://github.com/kornia/kornia/issues/1134 is resolved
            #  Basically, this whole wrapper function can be removed and `torch.testing.assert_close` can be used
            #  directly.
            rtol, atol = _default_tolerances(actual, expected)
    
>       return _assert_close(
            actual,
            expected,
            rtol=rtol,
            atol=atol,
            # this is the default value for torch>=1.10, but not for torch==1.9
            # TODO: remove this if kornia relies on torch>=1.10
            check_stride=False,
            equal_nan=False,
            **kwargs,
        )
E       AssertionError: Scalars are not close!
E       
E       Expected 0.3032509982585907 but got 0.0.
E       Absolute difference: 0.3032509982585907 (up to 1e-05 allowed)
E       Relative difference: 1.0 (up to 0.0001 allowed)

../publishablew/kornia/kornia/testing/base.py:37: AssertionError
=========================== short test summary info ============================
FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574]
FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574]
FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251]
FAILED ../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251]
========================= 4 failed, 20 passed in 2.10s =========================


Final Test Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'jit', 'cudagraphs', 'openxla', 'tvm', 'onnxrt', 'inductor', None}
model weights cached: ['checkpoints']

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1
collecting ... collected 24 items

../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred0-target0-expected0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred1-target1-expected1] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred2-target2-expected2] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred3-target3-expected3] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_kl[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_js[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_kl[cpu-float32-inductor] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_js[cpu-float32-inductor] PASSED

============================== 24 passed in 2.06s ==============================


Initial Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'inductor', 'tvm', 'jit', 'cudagraphs', 'onnxrt', None, 'openxla'}
model weights cached: ['checkpoints']

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1
collecting ... collected 24 items

../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred2-target2-0.346574] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_js_div_loss_2d[cpu-float32-pred3-target3-0.346574] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred0-target0-expected0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred1-target1-expected1] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred2-target2-expected2] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_kl_div_loss_2d_without_reduction[cpu-float32-pred3-target3-expected3] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred2-target2-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_kl[cpu-float32-pred3-target3-inf] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred0-target0-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred1-target1-0.0] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred2-target2-0.303251] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_noncontiguous_js[cpu-float32-pred3-target3-0.303251] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_kl[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_gradcheck_js[cpu] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_kl[cpu-float32-inductor] PASSED
../publishablew/kornia/kornia/tests/losses/test_divergence.py::TestDivergenceLoss::test_dynamo_js[cpu-float32-inductor] PASSED

============================== 24 passed in 2.24s ==============================
