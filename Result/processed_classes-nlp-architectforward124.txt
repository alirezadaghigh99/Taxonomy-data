output file:
processed_classes-nlp-architectforward124.json
function:
forward
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward FAILED [ 96%]', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward FAILED [100%]', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema FAILED [ 72%]', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization', 'FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward FAILED [ 16%]', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward FAILED [  8%]', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward FAILED [ 12%]', '../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization FAILED [ 20%]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.7.17, pytest-7.4.4, pluggy-1.2.0 -- /local/data0/moved_data/publishablew/nlp-architect/nlp-architect/nvenv/bin/python3
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/nlp-architect/nlp-architect
configfile: pytest.ini
collecting ... collected 25 items

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_backward PASSED [  4%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward FAILED [  8%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward FAILED [ 12%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward FAILED [ 16%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization FAILED [ 20%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization_data_parallel PASSED [ 24%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_with_bias PASSED [ 28%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_without_bias PASSED [ 32%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_with_bias PASSED [ 36%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_without_bias PASSED [ 40%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_none_quantized_linear PASSED [ 44%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_restrict_loading_to_train_model PASSED [ 48%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay PASSED [ 52%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel FAILED [ 56%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_static_quantized_inference PASSED [ 60%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_train_block_when_loading_quantized_model PASSED [ 64%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_dynamic PASSED [ 68%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema FAILED [ 72%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_delay_quantization_start PASSED [ 76%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_export_to_8bit PASSED [ 80%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_load_from_8bit PASSED [ 84%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantization_turned_off PASSED [ 88%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_backward PASSED [ 92%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward FAILED [ 96%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward FAILED [100%]

=================================== FAILURES ===================================
________ FakeLinearQuantizationWithSTETester.test_quantization_forward _________

self = <tests.test_quantization.FakeLinearQuantizationWithSTETester testMethod=test_quantization_forward>

    def test_quantization_forward(self):
        fake_quantize = FakeLinearQuantizationWithSTE().apply
        x = torch.randn(1, 100)
        scale = (2 ** (8 - 1) - 1) / np.abs(x).max()
>       self.assertTrue((fake_quantize(x, scale, 8) == fake_quantize_np(x, scale, 8)).all())
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:45: AssertionError
__________ QuantizedLinearTest.test_dynamic_quantized_linear_backward __________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_dynamic_quantized_linear_backward>

    def test_dynamic_quantized_linear_backward(self):
        x = torch.randn(1, 100, requires_grad=True)
        linear = QuantizedLinear(100, 1, bias=False, mode="DYNAMIC")
        y = linear(x)
        y.backward()
        self.assertTrue((x.grad == linear.fake_quantized_weight).all())
        with torch.no_grad():
            scale = (2 ** (8 - 1) - 1) / x.abs().max()
>       self.assertTrue((fake_quantize_np(x.detach(), scale, 8) == linear.weight.grad).all())
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:200: AssertionError
__________ QuantizedLinearTest.test_dynamic_quantized_linear_forward ___________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_dynamic_quantized_linear_forward>

    def test_dynamic_quantized_linear_forward(self):
        """Test QuantizedLinear forward method by giving in the input and
        weight values that are already quantized, therefore the quantization
        step should have no effect on the values and we know what values
        are expected"""
        x = torch.randn(1, 100).mul(127.0).round().clamp(-127.0, 127.0)
        qlinear = QuantizedLinear(100, 1, bias=False, requantize_output=False, mode="dynamic")
        with torch.no_grad():
            scale = 127.0 / qlinear.weight.abs().max()
        self.assertTrue(
            (
>               qlinear.fake_quantized_weight == fake_quantize_np(qlinear.weight.detach(), scale, 8)
            ).all()
        )
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:69: AssertionError
__________________ QuantizedLinearTest.test_ema_quantization ___________________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_ema_quantization>

    def test_ema_quantization(self):
        ema_decay = 0.9
        qlinear = QuantizedLinear(10, 5, bias=False, ema_decay=ema_decay, mode="EMA")
        for i in range(5):
            x = torch.randn(3, 10)
            tmp_input_thresh = x.abs().max()
            if i == 0:
                input_ema = tmp_input_thresh
            else:
                input_ema -= (1 - ema_decay) * (input_ema - tmp_input_thresh)
            y = (
                fake_quantize_np(x, get_scale(8, input_ema), 8) @ qlinear.fake_quantized_weight.t()
            ).detach()
            tmp_output_thresh = y.abs().max()
            if i == 0:
                output_ema = tmp_output_thresh
            else:
                output_ema -= (1 - ema_decay) * (output_ema - tmp_output_thresh)
            y = fake_quantize_np(y, get_scale(8, output_ema), 8)
            y_hat = qlinear(x)
>           self.assertTrue((y == y_hat).all())
E           AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:128: AssertionError
_______ QuantizedLinearTest.test_start_quantization_delay_data_parallel ________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_start_quantization_delay_data_parallel>

    def test_start_quantization_delay_data_parallel(self):
        if not torch.cuda.is_available():
            return
        quantization_delay = 2
        qlinear = QuantizedLinear(10, 5, start_step=quantization_delay, mode="DYNAMIC")
        linear = nn.Linear(10, 5)
        linear.weight.data = qlinear.weight
        linear.bias.data = qlinear.bias
        qlinear = nn.DataParallel(qlinear).cuda()
        linear = nn.DataParallel(linear).cuda()
        for _ in range(quantization_delay):
            x = torch.randn(3, 10).cuda()
>           qy = qlinear(x)

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:150: in forward
    return self.module(*inputs[0], **kwargs[0])
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nlp_architect/nn/torch/quantization.py:90: in forward
    out = super().forward(input)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/linear.py:87: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[-1.0871, -0.9881, -0.9635,  0.6074,  1.0120,  0.2120, -1.0088,  0.0862,
         -0.4029, -1.1876],
        [... [-2.2288,  2.2462,  0.4933, -0.6366,  0.2716,  0.0451,  0.8801, -0.0112,
          0.7009,  1.5104]], device='cuda:0')
weight = Parameter containing:
tensor([[ 0.0718, -0.0077, -0.2551, -0.0914,  0.2740,  0.1491, -0.2522, -0.2547,
         -0.305... 0.1198, -0.0209,  0.2541, -0.0450,  0.0892,  0.1304,
         -0.0291,  0.2353]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([ 0.0384,  0.0424, -0.2391, -0.0111, -0.2318], device='cuda:0',
       requires_grad=True)

    def linear(input, weight, bias=None):
        # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` where `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if input.dim() == 2 and bias is not None:
            # fused op is marginally faster
>           ret = torch.addmm(bias, input, weight.t())
E           RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`

../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/functional.py:1370: RuntimeError
_______ QuantizedLinearTest.test_training_and_inference_differences_ema ________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_training_and_inference_differences_ema>

    def test_training_and_inference_differences_ema(self):
        qlinear = QuantizedLinear(10, 5, mode="EMA", bias=False)
        x = torch.randn(3, 10) * 2 + 0.1
        y = qlinear(x)
        qlinear.eval()
        y_hat = qlinear(x)
>       self.assertTrue((y - y_hat).norm() < 1e-6)
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:208: AssertionError
______ QuantizedEmbeddingTest.test_quantized_embedding_inference_forward _______

self = <tests.test_quantization.QuantizedEmbeddingTest testMethod=test_quantized_embedding_inference_forward>

    def test_quantized_embedding_inference_forward(self):
        embedding = QuantizedEmbedding(10, 3, mode="ema")
        with torch.no_grad():
            scale = 127.0 / embedding.weight.abs().max()
        self.assertTrue(
            (
                embedding.fake_quantized_weight
>               == fake_quantize_np(embedding.weight.detach(), scale, 8)
            ).all()
        )
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:363: AssertionError
_______ QuantizedEmbeddingTest.test_quantized_embedding_training_forward _______

self = <tests.test_quantization.QuantizedEmbeddingTest testMethod=test_quantized_embedding_training_forward>

    def test_quantized_embedding_training_forward(self):
        embedding = QuantizedEmbedding(10, 3, mode="ema")
        with torch.no_grad():
            scale = 127.0 / embedding.weight.abs().max()
        self.assertTrue(
            (
                embedding.fake_quantized_weight
>               == fake_quantize_np(embedding.weight.detach(), scale, 8)
            ).all()
        )
E       AssertionError: tensor(False) is not true

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:345: AssertionError
=========================== short test summary info ============================
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward
========================= 8 failed, 17 passed in 2.32s =========================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.7.17, pytest-7.4.4, pluggy-1.2.0 -- /local/data0/moved_data/publishablew/nlp-architect/nlp-architect/nvenv/bin/python3
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/nlp-architect/nlp-architect
configfile: pytest.ini
collecting ... collected 25 items

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_backward PASSED [  4%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward PASSED [  8%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward PASSED [ 12%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward PASSED [ 16%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization PASSED [ 20%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization_data_parallel PASSED [ 24%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_with_bias PASSED [ 28%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_without_bias PASSED [ 32%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_with_bias PASSED [ 36%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_without_bias PASSED [ 40%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_none_quantized_linear PASSED [ 44%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_restrict_loading_to_train_model PASSED [ 48%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay PASSED [ 52%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel FAILED [ 56%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_static_quantized_inference PASSED [ 60%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_train_block_when_loading_quantized_model PASSED [ 64%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_dynamic PASSED [ 68%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema PASSED [ 72%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_delay_quantization_start PASSED [ 76%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_export_to_8bit PASSED [ 80%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_load_from_8bit PASSED [ 84%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantization_turned_off PASSED [ 88%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_backward PASSED [ 92%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward PASSED [ 96%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward PASSED [100%]

=================================== FAILURES ===================================
_______ QuantizedLinearTest.test_start_quantization_delay_data_parallel ________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_start_quantization_delay_data_parallel>

    def test_start_quantization_delay_data_parallel(self):
        if not torch.cuda.is_available():
            return
        quantization_delay = 2
        qlinear = QuantizedLinear(10, 5, start_step=quantization_delay, mode="DYNAMIC")
        linear = nn.Linear(10, 5)
        linear.weight.data = qlinear.weight
        linear.bias.data = qlinear.bias
        qlinear = nn.DataParallel(qlinear).cuda()
        linear = nn.DataParallel(linear).cuda()
        for _ in range(quantization_delay):
            x = torch.randn(3, 10).cuda()
>           qy = qlinear(x)

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:150: in forward
    return self.module(*inputs[0], **kwargs[0])
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nlp_architect/nn/torch/quantization.py:122: in forward
    out = super().forward(input)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/linear.py:87: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[-0.6983,  1.1886, -0.3994, -1.5956, -0.4771,  0.5858,  0.7946,  1.3120,
         -0.1970, -1.2632],
        [... [-0.4750,  0.1282, -0.2940,  1.0533,  1.1074, -0.2049, -0.1970, -0.8324,
         -0.6218,  0.3259]], device='cuda:0')
weight = Parameter containing:
tensor([[ 0.2460,  0.0759, -0.0320,  0.0245, -0.1852, -0.2708,  0.2060, -0.0102,
         -0.258... 0.0386,  0.1287,  0.0978,  0.3044,  0.1405,  0.0205,
         -0.0096, -0.1365]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([ 0.1596,  0.1501, -0.2933,  0.2217,  0.1935], device='cuda:0',
       requires_grad=True)

    def linear(input, weight, bias=None):
        # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` where `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if input.dim() == 2 and bias is not None:
            # fused op is marginally faster
>           ret = torch.addmm(bias, input, weight.t())
E           RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`

../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/functional.py:1370: RuntimeError
=========================== short test summary info ============================
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel
========================= 1 failed, 24 passed in 2.12s =========================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.7.17, pytest-7.4.4, pluggy-1.2.0 -- /local/data0/moved_data/publishablew/nlp-architect/nlp-architect/nvenv/bin/python3
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/nlp-architect/nlp-architect
configfile: pytest.ini
collecting ... collected 25 items

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_backward PASSED [  4%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::FakeLinearQuantizationWithSTETester::test_quantization_forward PASSED [  8%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_backward PASSED [ 12%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_dynamic_quantized_linear_forward PASSED [ 16%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization PASSED [ 20%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_ema_quantization_data_parallel PASSED [ 24%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_with_bias PASSED [ 28%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_export_to_8bit_without_bias PASSED [ 32%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_with_bias PASSED [ 36%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_import_from_8bit_without_bias PASSED [ 40%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_none_quantized_linear PASSED [ 44%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_restrict_loading_to_train_model PASSED [ 48%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay PASSED [ 52%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel FAILED [ 56%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_static_quantized_inference PASSED [ 60%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_train_block_when_loading_quantized_model PASSED [ 64%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_dynamic PASSED [ 68%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_training_and_inference_differences_ema PASSED [ 72%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_delay_quantization_start PASSED [ 76%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_export_to_8bit PASSED [ 80%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_load_from_8bit PASSED [ 84%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantization_turned_off PASSED [ 88%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_backward PASSED [ 92%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_inference_forward PASSED [ 96%]
../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedEmbeddingTest::test_quantized_embedding_training_forward PASSED [100%]

=================================== FAILURES ===================================
_______ QuantizedLinearTest.test_start_quantization_delay_data_parallel ________

self = <tests.test_quantization.QuantizedLinearTest testMethod=test_start_quantization_delay_data_parallel>

    def test_start_quantization_delay_data_parallel(self):
        if not torch.cuda.is_available():
            return
        quantization_delay = 2
        qlinear = QuantizedLinear(10, 5, start_step=quantization_delay, mode="DYNAMIC")
        linear = nn.Linear(10, 5)
        linear.weight.data = qlinear.weight
        linear.bias.data = qlinear.bias
        qlinear = nn.DataParallel(qlinear).cuda()
        linear = nn.DataParallel(linear).cuda()
        for _ in range(quantization_delay):
            x = torch.randn(3, 10).cuda()
>           qy = qlinear(x)

../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:150: in forward
    return self.module(*inputs[0], **kwargs[0])
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/module.py:532: in __call__
    result = self.forward(*input, **kwargs)
../publishablew/nlp-architect/nlp-architect/nlp_architect/nn/torch/quantization.py:122: in forward
    out = super().forward(input)
../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/modules/linear.py:87: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[ 0.4875,  0.0406, -1.6620, -1.9724,  1.5731,  0.2834,  0.3264, -1.9780,
         -0.2787, -0.2098],
        [... [-0.0563, -1.0617, -0.0622, -0.2473, -0.9945, -0.5464, -0.2258,  1.8460,
          1.2963,  0.8894]], device='cuda:0')
weight = Parameter containing:
tensor([[ 0.2230,  0.3117, -0.1892, -0.1471, -0.3158,  0.0483, -0.2610, -0.0005,
         -0.044...-0.2363, -0.1342, -0.2102,  0.1952, -0.1589, -0.1103,
         -0.0804,  0.0843]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([ 0.0529, -0.0634, -0.0135,  0.2167, -0.1074], device='cuda:0',
       requires_grad=True)

    def linear(input, weight, bias=None):
        # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` where `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if input.dim() == 2 and bias is not None:
            # fused op is marginally faster
>           ret = torch.addmm(bias, input, weight.t())
E           RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`

../publishablew/nlp-architect/nlp-architect/nvenv/lib/python3.7/site-packages/torch/nn/functional.py:1370: RuntimeError
=========================== short test summary info ============================
FAILED ../publishablew/nlp-architect/nlp-architect/tests/test_quantization.py::QuantizedLinearTest::test_start_quantization_delay_data_parallel
========================= 1 failed, 24 passed in 2.19s =========================
