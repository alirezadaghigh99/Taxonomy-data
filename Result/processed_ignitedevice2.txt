output file:
processed_ignitedevice2.json
function:
device
Error Cases:
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
[rank0]:[W116 20:49:56.799467946 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
[rank0]:[W116 20:49:59.507928490 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
[rank0]:[W116 20:50:01.229005008 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Pass or Failed: 0

Related Failed Test Cases:
{'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None]', 'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334]', 'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE]', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE] FAILED', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334] FAILED', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None] FAILED', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None] FAILED', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE] FAILED', 'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE]', 'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334]', '../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334] FAILED', 'FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/ignite/ignite/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/ignite/ignite
configfile: setup.cfg
collecting ... collected 49 items

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE] FAILED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first_asserts PASSED

==================================== ERRORS ====================================
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 109
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_sync_as_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 117
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_sync_as_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 126
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_new_group_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 134
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_new_group_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_gloo_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 153
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_nccl_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 161
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_gloo_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 190
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context_set_local_rank(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_nccl_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 198
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context_set_local_rank(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_nccl _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 207
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist__model_methods_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_gloo _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 215
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist__model_methods_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 222
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_reduce_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 231
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_reduce_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 239
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 249
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_nccl _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 258
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_gather_tensors_with_shapes_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_gloo _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 267
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_gather_tensors_with_shapes_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 275
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_broadcast_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 283
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_broadcast_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_nccl ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 290
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_barrier_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_gloo ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 298
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_barrier_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_gloo ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 332
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(
      not torch.cuda.is_available(), reason="Do not want to run this test on Github or Travis, but CircleCI"
  )
  def test_idist_methods_overhead_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_nccl ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 349
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_overhead_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_gloo ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 364
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_one_rank_only_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_nccl ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 372
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_one_rank_only_nccl(local_rank, distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[nccl-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[nccl-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
____________ ERROR at setup of test_one_rank_first[gloo_cpu-True-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___________ ERROR at setup of test_one_rank_first[gloo_cpu-False-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[gloo-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[gloo-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
=================================== FAILURES ===================================
_______________ test_native_distrib_single_node_spawn_gloo[None] _______________

init_method = None, dirname = PosixPath('/tmp/tmpkwl37y2y')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_gloo(init_method, dirname):
        from datetime import timedelta
    
        timeout = timedelta(seconds=20)
    
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>       _test_native_distrib_single_node_spawn(init_method, "gloo", device, timeout=timeout)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83dc8fc50>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 20, in device
E           return torch.device1('cpu')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
_______ test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334] ________

init_method = 'tcp://0.0.0.0:22334', dirname = PosixPath('/tmp/tmpm65br0zw')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_gloo(init_method, dirname):
        from datetime import timedelta
    
        timeout = timedelta(seconds=20)
    
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>       _test_native_distrib_single_node_spawn(init_method, "gloo", device, timeout=timeout)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83ebda310>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 20, in device
E           return torch.device1('cpu')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
_______________ test_native_distrib_single_node_spawn_gloo[FILE] _______________

init_method = 'file:///tmp/tmp1t9x7k6u/shared'
dirname = PosixPath('/tmp/tmp1t9x7k6u')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_gloo(init_method, dirname):
        from datetime import timedelta
    
        timeout = timedelta(seconds=20)
    
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>       _test_native_distrib_single_node_spawn(init_method, "gloo", device, timeout=timeout)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83dc1cdd0>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 20, in device
E           return torch.device1('cpu')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
_______________ test_native_distrib_single_node_spawn_nccl[None] _______________

init_method = None, dirname = PosixPath('/tmp/tmp3owcj8p9')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_nccl(init_method, dirname):
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda")
>       _test_native_distrib_single_node_spawn(init_method, "nccl", device)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83dbe8610>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 18, in device
E           return torch.device1(f'cuda:{local_rank}')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
_______ test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334] ________

init_method = 'tcp://0.0.0.0:22334', dirname = PosixPath('/tmp/tmp8y2m6_v1')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_nccl(init_method, dirname):
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda")
>       _test_native_distrib_single_node_spawn(init_method, "nccl", device)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83d135b50>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 18, in device
E           return torch.device1(f'cuda:{local_rank}')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
_______________ test_native_distrib_single_node_spawn_nccl[FILE] _______________

init_method = 'file:///tmp/tmp3ve2puhz/shared'
dirname = PosixPath('/tmp/tmp3ve2puhz')

    @pytest.mark.distributed
    @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
    @pytest.mark.skipif("WORLD_SIZE" in os.environ, reason="Skip if launched as multiproc")
    @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
    @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
    def test_native_distrib_single_node_spawn_nccl(init_method, dirname):
        if init_method == "FILE":
            init_method = f"file://{dirname}/shared"
    
        device = torch.device("cuda")
>       _test_native_distrib_single_node_spawn(init_method, "nccl", device)

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py:70: in _test_native_distrib_single_node_spawn
    idist.spawn(
../publishablew/ignite/ignite/ignite/distributed/utils.py:255: in spawn
    comp_model_cls.spawn(fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs)
../publishablew/ignite/ignite/ignite/distributed/comp_models/native.py:392: in spawn
    start_processes(
../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:284: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.multiprocessing.spawn.ProcessContext object at 0x7ba83cd1d910>
timeout = 30

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        # Try SIGTERM then SIGKILL if the process isn't going down.
        # The reason is related to python signal handling is limited
        # to main thread and if that is in c/c++ land and stuck it won't
        # to handle it. We have seen processes getting stuck not handling
        # SIGTERM for the above reason.
        timeout: int = 30
        for process in self.processes:
            if process.is_alive():
                log.warning("Terminating process %s via signal SIGTERM", process.pid)
                process.terminate()
        end = time.monotonic() + timeout
        for process in self.processes:
            time_to_wait = max(0, end - time.monotonic())
            process.join(time_to_wait)
        for process in self.processes:
            if process.is_alive():
                log.warning(
                    "Unable to shutdown process %s via SIGTERM , forcefully exiting via SIGKILL",
                    process.pid,
                )
                process.kill()
            process.join()
    
        # The file will only be created if the process crashed.
        failed_process = self.processes[error_index]
        if not os.access(self.error_files[error_index], os.R_OK):
            exitcode = self.processes[error_index].exitcode
            if exitcode < 0:
                try:
                    name = signal.Signals(-exitcode).name
                except ValueError:
                    name = f"<Unknown signal {-exitcode}>"
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        with open(self.error_files[error_index], "rb") as fh:
            original_trace = pickle.load(fh)
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
E           fn(i, *args)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
E           fn(local_rank, *args, **kw_dict)
E         File "/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/__init__.py", line 22, in _test_distrib_config
E           this_device = idist.device()
E                         ^^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/utils.py", line 34, in device
E           return device()
E                  ^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/ignite/distributed/temp.py", line 18, in device
E           return torch.device1(f'cuda:{local_rank}')
E                  ^^^^^^^^^^^^^
E         File "/local/data0/moved_data/publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/__init__.py", line 2562, in __getattr__
E           raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
E       AttributeError: module 'torch' has no attribute 'device1'

../publishablew/ignite/ignite/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:203: ProcessRaisedException
=============================== warnings summary ===============================
../publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16
  /local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
    from torch.distributed.optim import ZeroRedundancyOptimizer

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None]
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334]
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE]
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None]
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334]
FAILED ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0]
======== 6 failed, 1 passed, 6 skipped, 1 warning, 36 errors in 14.23s =========


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/ignite/ignite/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/ignite/ignite
configfile: setup.cfg
collecting ... collected 49 items

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first_asserts PASSED

==================================== ERRORS ====================================
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 109
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_sync_as_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 117
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_sync_as_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 126
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_new_group_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 134
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_new_group_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_gloo_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 153
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_nccl_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 161
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_gloo_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 190
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context_set_local_rank(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_nccl_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 198
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context_set_local_rank(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_nccl _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 207
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist__model_methods_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_gloo _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 215
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist__model_methods_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 222
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_reduce_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 231
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_reduce_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 239
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 249
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_nccl _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 258
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_gather_tensors_with_shapes_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_gloo _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 267
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_gather_tensors_with_shapes_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 275
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_broadcast_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 283
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_broadcast_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_nccl ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 290
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_barrier_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_gloo ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 298
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_barrier_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_gloo ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 332
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(
      not torch.cuda.is_available(), reason="Do not want to run this test on Github or Travis, but CircleCI"
  )
  def test_idist_methods_overhead_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_nccl ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 349
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_overhead_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_gloo ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 364
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_one_rank_only_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_nccl ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 372
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_one_rank_only_nccl(local_rank, distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[nccl-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[nccl-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
____________ ERROR at setup of test_one_rank_first[gloo_cpu-True-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___________ ERROR at setup of test_one_rank_first[gloo_cpu-False-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[gloo-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[gloo-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
=============================== warnings summary ===============================
../publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16
  /local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
    from torch.distributed.optim import ZeroRedundancyOptimizer

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0]
============= 7 passed, 6 skipped, 1 warning, 36 errors in 13.94s ==============


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/ignite/ignite/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/ignite/ignite
configfile: setup.cfg
collecting ... collected 49 items

../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[None] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[tcp://0.0.0.0:22334] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_gloo[FILE] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[None] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[tcp://0.0.0.0:22334] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_spawn_nccl[FILE] PASSED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0] ERROR
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[horovod-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[single_device_xla-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-True-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[xla_nprocs-False-0] SKIPPED
../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first_asserts PASSED

==================================== ERRORS ====================================
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_gloo[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 39
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_gloo(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[None] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334] _
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___ ERROR at setup of test_native_distrib_single_node_launch_tool_nccl[FILE] ___
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 56
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.parametrize("init_method", [None, "tcp://0.0.0.0:22334", "FILE"])
  def test_native_distrib_single_node_launch_tool_nccl(init_method, get_fixed_dirname, local_rank, world_size):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 109
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_sync_as_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_sync_as_native_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 117
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_sync_as_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 126
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_new_group_native_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_new_group_native_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 134
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_new_group_native_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_gloo_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 153
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________ ERROR at setup of test_idist_methods_in_native_nccl_context __________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 161
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_gloo_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 190
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_methods_in_native_gloo_context_set_local_rank(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__ ERROR at setup of test_idist_methods_in_native_nccl_context_set_local_rank __
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 198
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_in_native_nccl_context_set_local_rank(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_nccl _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 207
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist__model_methods_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist__model_methods_gloo _______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 215
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist__model_methods_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 222
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_reduce_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_reduce_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 231
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_reduce_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_nccl _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 239
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_all_gather_gloo _________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 249
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(Version(torch.__version__) < Version("1.7.0"), reason="dist.all_gather_object is not implemented")
  def test_idist_all_gather_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_nccl _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 258
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_all_gather_tensors_with_shapes_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______ ERROR at setup of test_idist_all_gather_tensors_with_shapes_gloo _______
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 267
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_all_gather_tensors_with_shapes_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_nccl __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 275
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_broadcast_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_________________ ERROR at setup of test_idist_broadcast_gloo __________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 283
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_broadcast_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_nccl ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 290
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_barrier_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
__________________ ERROR at setup of test_idist_barrier_gloo ___________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 298
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_barrier_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_gloo ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 332
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(
      not torch.cuda.is_available(), reason="Do not want to run this test on Github or Travis, but CircleCI"
  )
  def test_idist_methods_overhead_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_idist_methods_overhead_nccl ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 349
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_methods_overhead_nccl(distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_gloo ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 364
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  def test_idist_one_rank_only_gloo(distributed_context_single_node_gloo):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_______________ ERROR at setup of test_idist_one_rank_only_nccl ________________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 372
  @pytest.mark.distributed
  @pytest.mark.skipif(not has_native_dist_support, reason="Skip if no native dist support")
  @pytest.mark.skipif(torch.cuda.device_count() < 1, reason="Skip if no GPU")
  def test_idist_one_rank_only_nccl(local_rank, distributed_context_single_node_nccl):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[nccl-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[nccl-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
____________ ERROR at setup of test_one_rank_first[gloo_cpu-True-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
___________ ERROR at setup of test_one_rank_first[gloo_cpu-False-0] ____________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
______________ ERROR at setup of test_one_rank_first[gloo-True-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
_____________ ERROR at setup of test_one_rank_first[gloo-False-0] ______________
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py, line 381
  @pytest.mark.distributed
  @pytest.mark.parametrize("rank", range(int(os.environ.get("WORLD_SIZE", 1))))
  @pytest.mark.parametrize("local", [True, False])
  def test_one_rank_first(distributed, get_rank_zero_dirname, rank, local):
file /local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py, line 121
  @pytest.fixture(scope="module")
  def local_rank(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: available_device, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, clean_env, dirname, distributed, distributed_context_multi_node_gloo, distributed_context_multi_node_nccl, distributed_context_single_node_gloo, distributed_context_single_node_nccl, doctest_namespace, get_fixed_dirname, get_rank_zero_dirname, gloo_hvd_executor, local_rank, mock_gpu_is_not_available, monkeypatch, multi_node_conf, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, term_handler, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, world_size, xmp_executor
>       use 'pytest --fixtures [testpath]' for help on them.

/local/data0/moved_data/publishablew/ignite/ignite/tests/ignite/conftest.py:121
=============================== warnings summary ===============================
../publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16
  /local/data0/moved_data/publishablew/ignite/ignite/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
    from torch.distributed.optim import ZeroRedundancyOptimizer

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_gloo[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[None]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[tcp://0.0.0.0:22334]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_native_distrib_single_node_launch_tool_nccl[FILE]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_sync_as_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_new_group_native_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_gloo_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_in_native_nccl_context_set_local_rank
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist__model_methods_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_reduce_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_all_gather_tensors_with_shapes_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_broadcast_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_barrier_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_methods_overhead_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_gloo
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_idist_one_rank_only_nccl
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[nccl-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo_cpu-False-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-True-0]
ERROR ../publishablew/ignite/ignite/tests/ignite/distributed/utils/test_native.py::test_one_rank_first[gloo-False-0]
============= 7 passed, 6 skipped, 1 warning, 36 errors in 15.01s ==============
