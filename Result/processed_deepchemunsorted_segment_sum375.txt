output file:
processed_deepchemunsorted_segment_sum375.json
function:
unsorted_segment_sum
Error Cases:
2024-12-26 19:22:25.660036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1735258945.671462  892157 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1735258945.674869  892157 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-26 19:22:25.687890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

Pass or Failed: 1

Related Failed Test Cases:
set()

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/deepchem/deepchem/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/deepchem/deepchem
configfile: setup.cfg
plugins: flaky-3.8.1
collecting ... collected 12 items

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_chunkify PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_memory PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_gaussian_integral PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_TensorNonTensorSeparator PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_tallqr PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_to_fortran_order PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_np_dtype PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_estimate_ovlp_rcut PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_dtype_memsize PASSED

=================================== FAILURES ===================================
__________________________ test_unsorted_segment_sum ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_sum():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_sum(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
_______________________________ test_segment_sum _______________________________

    @pytest.mark.torch
    def test_segment_sum():
    
        data = torch.Tensor([[1, 2, 3, 4], [4, 3, 2, 1], [5, 6, 7, 8]])
        segment_ids = torch.Tensor([0, 0, 1]).to(torch.int64)
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.segment_sum(data=data,
                                                    segment_ids=segment_ids)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
__________________________ test_unsorted_segment_max ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_max():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_max(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_max.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_max.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_max.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
=========================== short test summary info ============================
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max
========================= 3 failed, 9 passed in 4.12s ==========================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/deepchem/deepchem/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/deepchem/deepchem
configfile: setup.cfg
plugins: flaky-3.8.1
collecting ... collected 12 items

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_chunkify PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_memory PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_gaussian_integral PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_TensorNonTensorSeparator PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_tallqr PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_to_fortran_order PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_np_dtype PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_estimate_ovlp_rcut PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_dtype_memsize PASSED

=================================== FAILURES ===================================
__________________________ test_unsorted_segment_sum ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_sum():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_sum(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
_______________________________ test_segment_sum _______________________________

    @pytest.mark.torch
    def test_segment_sum():
    
        data = torch.Tensor([[1, 2, 3, 4], [4, 3, 2, 1], [5, 6, 7, 8]])
        segment_ids = torch.Tensor([0, 0, 1]).to(torch.int64)
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.segment_sum(data=data,
                                                    segment_ids=segment_ids)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
__________________________ test_unsorted_segment_max ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_max():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_max(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_max.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_max.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_max.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
=========================== short test summary info ============================
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max
========================= 3 failed, 9 passed in 4.13s ==========================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/deepchem/deepchem/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/deepchem/deepchem
configfile: setup.cfg
plugins: flaky-3.8.1
collecting ... collected 12 items

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_chunkify PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_memory PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_gaussian_integral PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_TensorNonTensorSeparator PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_tallqr PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_to_fortran_order PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_np_dtype PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max FAILED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_estimate_ovlp_rcut PASSED
../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_get_dtype_memsize PASSED

=================================== FAILURES ===================================
__________________________ test_unsorted_segment_sum ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_sum():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_sum(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
_______________________________ test_segment_sum _______________________________

    @pytest.mark.torch
    def test_segment_sum():
    
        data = torch.Tensor([[1, 2, 3, 4], [4, 3, 2, 1], [5, 6, 7, 8]])
        segment_ids = torch.Tensor([0, 0, 1]).to(torch.int64)
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.segment_sum(data=data,
                                                    segment_ids=segment_ids)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_sum.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_sum.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_sum.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
__________________________ test_unsorted_segment_max ___________________________

    @pytest.mark.torch
    def test_unsorted_segment_max():
    
        segment_ids = torch.Tensor([0, 1, 0]).to(torch.int64)
        data = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [4, 3, 2, 1]])
        num_segments = 2
    
        if len(segment_ids.shape) != 1:
            raise AssertionError("segment_ids have be a 1-D tensor")
    
        if data.shape[0] != segment_ids.shape[0]:
            raise AssertionError(
                "segment_ids should be the same size as dimension 0 of input.")
    
        result = dc.utils.pytorch_utils.unsorted_segment_max(
            data=data, segment_ids=segment_ids, num_segments=num_segments)
    
        assert np.allclose(
            np.array(result),
>           np.load("deepchem/utils/test/assets/result_segment_max.npy"),
            atol=1e-04)

../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

file = 'deepchem/utils/test/assets/result_segment_max.npy', mmap_mode = None
allow_pickle = False, fix_imports = True, encoding = 'ASCII'

    @set_module('numpy')
    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,
             encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):
        """
        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.
    
        .. warning:: Loading files that contain object arrays uses the ``pickle``
                     module, which is not secure against erroneous or maliciously
                     constructed data. Consider passing ``allow_pickle=False`` to
                     load data that is known not to contain object arrays for the
                     safer handling of untrusted sources.
    
        Parameters
        ----------
        file : file-like object, string, or pathlib.Path
            The file to read. File-like objects must support the
            ``seek()`` and ``read()`` methods and must always
            be opened in binary mode.  Pickled files require that the
            file-like object support the ``readline()`` method as well.
        mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional
            If not None, then memory-map the file, using the given mode (see
            `numpy.memmap` for a detailed description of the modes).  A
            memory-mapped array is kept on disk. However, it can be accessed
            and sliced like any ndarray.  Memory mapping is especially useful
            for accessing small fragments of large files without reading the
            entire file into memory.
        allow_pickle : bool, optional
            Allow loading pickled object arrays stored in npy files. Reasons for
            disallowing pickles include security, as loading pickled data can
            execute arbitrary code. If pickles are disallowed, loading object
            arrays will fail. Default: False
    
            .. versionchanged:: 1.16.3
                Made default False in response to CVE-2019-6446.
    
        fix_imports : bool, optional
            Only useful when loading Python 2 generated pickled files on Python 3,
            which includes npy/npz files containing object arrays. If `fix_imports`
            is True, pickle will try to map the old Python 2 names to the new names
            used in Python 3.
        encoding : str, optional
            What encoding to use when reading Python 2 strings. Only useful when
            loading Python 2 generated pickled files in Python 3, which includes
            npy/npz files containing object arrays. Values other than 'latin1',
            'ASCII', and 'bytes' are not allowed, as they can corrupt numerical
            data. Default: 'ASCII'
        max_header_size : int, optional
            Maximum allowed size of the header.  Large headers may not be safe
            to load securely and thus require explicitly passing a larger value.
            See :py:func:`ast.literal_eval()` for details.
            This option is ignored when `allow_pickle` is passed.  In that case
            the file is by definition trusted and the limit is unnecessary.
    
        Returns
        -------
        result : array, tuple, dict, etc.
            Data stored in the file. For ``.npz`` files, the returned instance
            of NpzFile class must be closed to avoid leaking file descriptors.
    
        Raises
        ------
        OSError
            If the input file does not exist or cannot be read.
        UnpicklingError
            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.
        ValueError
            The file contains an object array, but ``allow_pickle=False`` given.
        EOFError
            When calling ``np.load`` multiple times on the same file handle,
            if all data has already been read
    
        See Also
        --------
        save, savez, savez_compressed, loadtxt
        memmap : Create a memory-map to an array stored in a file on disk.
        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
    
        Notes
        -----
        - If the file contains pickle data, then whatever object is stored
          in the pickle is returned.
        - If the file is a ``.npy`` file, then a single array is returned.
        - If the file is a ``.npz`` file, then a dictionary-like object is
          returned, containing ``{filename: array}`` key-value pairs, one for
          each file in the archive.
        - If the file is a ``.npz`` file, the returned value supports the
          context manager protocol in a similar fashion to the open function::
    
            with load('foo.npz') as data:
                a = data['a']
    
          The underlying file descriptor is closed when exiting the 'with'
          block.
    
        Examples
        --------
        Store data to disk, and load it again:
    
        >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))
        >>> np.load('/tmp/123.npy')
        array([[1, 2, 3],
               [4, 5, 6]])
    
        Store compressed data to disk, and load it again:
    
        >>> a=np.array([[1, 2, 3], [4, 5, 6]])
        >>> b=np.array([1, 2])
        >>> np.savez('/tmp/123.npz', a=a, b=b)
        >>> data = np.load('/tmp/123.npz')
        >>> data['a']
        array([[1, 2, 3],
               [4, 5, 6]])
        >>> data['b']
        array([1, 2])
        >>> data.close()
    
        Mem-map the stored array, and then access the second row
        directly from disk:
    
        >>> X = np.load('/tmp/123.npy', mmap_mode='r')
        >>> X[1, :]
        memmap([4, 5, 6])
    
        """
        if encoding not in ('ASCII', 'latin1', 'bytes'):
            # The 'encoding' value for pickle also affects what encoding
            # the serialized binary data of NumPy arrays is loaded
            # in. Pickle does not pass on the encoding information to
            # NumPy. The unpickling code in numpy.core.multiarray is
            # written to assume that unicode data appearing where binary
            # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.
            #
            # Other encoding values can corrupt binary data, and we
            # purposefully disallow them. For the same reason, the errors=
            # argument is not exposed, as values other than 'strict'
            # result can similarly silently corrupt numerical data.
            raise ValueError("encoding must be 'ASCII', 'latin1', or 'bytes'")
    
        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    
        with contextlib.ExitStack() as stack:
            if hasattr(file, 'read'):
                fid = file
                own_fid = False
            else:
>               fid = stack.enter_context(open(os_fspath(file), "rb"))
E               FileNotFoundError: [Errno 2] No such file or directory: 'deepchem/utils/test/assets/result_segment_max.npy'

../publishablew/deepchem/deepchem/venv/lib/python3.11/site-packages/numpy/lib/npyio.py:427: FileNotFoundError
=========================== short test summary info ============================
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_segment_sum
FAILED ../publishablew/deepchem/deepchem/deepchem/utils/test/test_pytorch_utils.py::test_unsorted_segment_max
========================= 3 failed, 9 passed in 4.48s ==========================
