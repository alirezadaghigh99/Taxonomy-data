output file:
processed_scikit-learnmake_scorer153.json
function:
make_scorer
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score]', "FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))]", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score] FAILED', "FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')]", 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling FAILED', "../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))] FAILED", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance] FAILED', "../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)] FAILED", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data FAILED', "FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')]", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score]', "FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)]", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio]', "../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')] FAILED", '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list]', "../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')] FAILED", 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score] FAILED', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance]', '../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score] FAILED', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro]', 'FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 273 items

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_all_scorers_repr I: Seeding RNGs with 1159678808
PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of callables] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[list of int] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of one callable] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-unique str] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-string key dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1-f1_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_weighted-metric1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_macro-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_micro-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision-precision_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall-recall_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard-jaccard_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_weighted-metric13] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_macro-metric14] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_micro-metric15] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[top_k_accuracy-top_k_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[matthews_corrcoef-matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[accuracy-accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[balanced_accuracy-balanced_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorers PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scoring_is_not_metric PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_sanity_check PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_return_copy PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_no_op_multiclass_select_proba PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request_raises_without_routing_enabled PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_repr PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer_pos_label[42] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning FAILED

=================================== FAILURES ===================================
________ test_check_scoring_and_check_multimetric_scoring[single_tuple] ________

scoring = ('accuracy',)

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
________ test_check_scoring_and_check_multimetric_scoring[single_list] _________

scoring = ['precision']

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
__________ test_check_scoring_and_check_multimetric_scoring[dict_str] __________

scoring = {'acc': 'accuracy', 'precision': 'precision'}

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
________ test_check_scoring_and_check_multimetric_scoring[multi_tuple] _________

scoring = ('accuracy', 'precision')

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
_________ test_check_scoring_and_check_multimetric_scoring[multi_list] _________

scoring = ['precision', 'accuracy']

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
_______ test_check_scoring_and_check_multimetric_scoring[dict_callable] ________

scoring = {'accuracy': <sklearn.metrics.temp.Scorer object at 0x70c8876cabb0>, 'precision': <sklearn.metrics.temp.Scorer object at 0x70c8876ca790>}

    @pytest.mark.parametrize(
        "scoring",
        (
            ("accuracy",),
            ["precision"],
            {"acc": "accuracy", "precision": "precision"},
            ("accuracy", "precision"),
            ["precision", "accuracy"],
            {
                "accuracy": make_scorer(accuracy_score),
                "precision": make_scorer(precision_score),
            },
        ),
        ids=[
            "single_tuple",
            "single_list",
            "dict_str",
            "multi_tuple",
            "multi_list",
            "dict_callable",
        ],
    )
    def test_check_scoring_and_check_multimetric_scoring(scoring):
>       check_scoring_validator_for_single_metric_usecases(check_scoring)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:239: in check_scoring_validator_for_single_metric_usecases
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
_______________________ test_check_scoring_gridsearchcv ________________________

    def test_check_scoring_gridsearchcv():
        # test that check_scoring works on GridSearchCV and pipeline.
        # slightly redundant non-regression test.
    
        grid = GridSearchCV(LinearSVC(), param_grid={"C": [0.1, 1]}, cv=3)
        scorer = check_scoring(grid, scoring="f1")
>       assert isinstance(scorer, _Scorer)
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:343: AssertionError
________ test_classification_binary_scores[precision_weighted-metric5] _________

scorer_name = 'precision_weighted'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(0.9230769230769231), np.float64(0.963076923076923))
kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(0.9230769230769231)
E            DESIRED: np.float64(0.963076923076923)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
__________ test_classification_binary_scores[precision_macro-metric6] __________

scorer_name = 'precision_macro'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(0.9230769230769231), np.float64(0.9615384615384616))
kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(0.9230769230769231)
E            DESIRED: np.float64(0.9615384615384616)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
__________ test_classification_binary_scores[precision_micro-metric7] __________

scorer_name = 'precision_micro'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(0.9230769230769231), np.float64(0.96)), kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(0.9230769230769231)
E            DESIRED: np.float64(0.96)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
__________ test_classification_binary_scores[recall_weighted-metric9] __________

scorer_name = 'recall_weighted'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(1.0), np.float64(0.96)), kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(1.0)
E            DESIRED: np.float64(0.96)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
___________ test_classification_binary_scores[recall_macro-metric10] ___________

scorer_name = 'recall_macro'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(1.0), np.float64(0.9615384615384616)), kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(1.0)
E            DESIRED: np.float64(0.9615384615384616)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
___________ test_classification_binary_scores[recall_micro-metric11] ___________

scorer_name = 'recall_micro'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("f1", f1_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision", precision_score),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall", recall_score),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard", jaccard_score),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
            ("top_k_accuracy", top_k_accuracy_score),
            ("matthews_corrcoef", matthews_corrcoef),
        ],
    )
    def test_classification_binary_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # binary classification.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        score = get_scorer(scorer_name)(clf, X_test, y_test)
        expected_score = metric(y_test, clf.predict(X_test))
>       assert_almost_equal(score, expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(1.0), np.float64(0.96)), kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(1.0)
E            DESIRED: np.float64(0.96)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
__________ test_classification_multiclass_scores[f1_weighted-metric2] __________

scorer_name = 'f1_weighted'
metric = functools.partial(<function f1_score at 0x70c891af3b80>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1289: in f1_score
    return fbeta_score(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1482: in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
___________ test_classification_multiclass_scores[f1_macro-metric3] ____________

scorer_name = 'f1_macro'
metric = functools.partial(<function f1_score at 0x70c891af3b80>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1289: in f1_score
    return fbeta_score(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1482: in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
___________ test_classification_multiclass_scores[f1_micro-metric4] ____________

scorer_name = 'f1_micro'
metric = functools.partial(<function f1_score at 0x70c891af3b80>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1289: in f1_score
    return fbeta_score(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1482: in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
______ test_classification_multiclass_scores[precision_weighted-metric5] _______

scorer_name = 'precision_weighted'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2209: in precision_score
    p, _, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
________ test_classification_multiclass_scores[precision_macro-metric6] ________

scorer_name = 'precision_macro'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2209: in precision_score
    p, _, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
________ test_classification_multiclass_scores[precision_micro-metric7] ________

scorer_name = 'precision_micro'
metric = functools.partial(<function precision_score at 0x70c891ae81f0>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2209: in precision_score
    p, _, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
________ test_classification_multiclass_scores[recall_weighted-metric8] ________

scorer_name = 'recall_weighted'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2391: in recall_score
    _, r, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_________ test_classification_multiclass_scores[recall_macro-metric9] __________

scorer_name = 'recall_macro'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2391: in recall_score
    _, r, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_________ test_classification_multiclass_scores[recall_micro-metric10] _________

scorer_name = 'recall_micro'
metric = functools.partial(<function recall_score at 0x70c891ae8310>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2391: in recall_score
    _, r, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_______ test_classification_multiclass_scores[jaccard_weighted-metric11] _______

scorer_name = 'jaccard_weighted'
metric = functools.partial(<function jaccard_score at 0x70c891af3820>, average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:888: in jaccard_score
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
________ test_classification_multiclass_scores[jaccard_macro-metric12] _________

scorer_name = 'jaccard_macro'
metric = functools.partial(<function jaccard_score at 0x70c891af3820>, average='macro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:888: in jaccard_score
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
________ test_classification_multiclass_scores[jaccard_micro-metric13] _________

scorer_name = 'jaccard_micro'
metric = functools.partial(<function jaccard_score at 0x70c891af3820>, average='micro')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("accuracy", accuracy_score),
            ("balanced_accuracy", balanced_accuracy_score),
            ("f1_weighted", partial(f1_score, average="weighted")),
            ("f1_macro", partial(f1_score, average="macro")),
            ("f1_micro", partial(f1_score, average="micro")),
            ("precision_weighted", partial(precision_score, average="weighted")),
            ("precision_macro", partial(precision_score, average="macro")),
            ("precision_micro", partial(precision_score, average="micro")),
            ("recall_weighted", partial(recall_score, average="weighted")),
            ("recall_macro", partial(recall_score, average="macro")),
            ("recall_micro", partial(recall_score, average="micro")),
            ("jaccard_weighted", partial(jaccard_score, average="weighted")),
            ("jaccard_macro", partial(jaccard_score, average="macro")),
            ("jaccard_micro", partial(jaccard_score, average="micro")),
        ],
    )
    def test_classification_multiclass_scores(scorer_name, metric):
        # check consistency between score and scorer for scores supporting
        # multiclass classification.
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=30, random_state=0
        )
    
        # use `stratify` = y to ensure train and test sets capture all classes
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=0, stratify=y
        )
    
        clf = DecisionTreeClassifier(random_state=0)
        clf.fit(X_train, y_train)
>       score = get_scorer(scorer_name)(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:888: in jaccard_score
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([0, 2, 0, 2, 1, 1, 1, 0])
y_pred = array([0, 2, 0, 2, 2, 1, 1, 2]), average = 'binary', labels = None
pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_________________________ test_custom_scorer_pickling __________________________

    def test_custom_scorer_pickling():
        # test that custom scorer can be pickled
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LinearSVC(random_state=0)
        clf.fit(X_train, y_train)
    
        scorer = make_scorer(fbeta_score, beta=2)
>       score1 = scorer(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:442: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:194: in wrapper
    params = func_sig.bind(*args, **kwargs)
/usr/local/lib/python3.9/inspect.py:3050: in bind
    return self._bind(args, kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>
args = (array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 1, 1]), array([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 1, 1]))
kwargs = {}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
>                           raise TypeError(msg) from None
E                           TypeError: missing a required argument: 'beta'

/usr/local/lib/python3.9/inspect.py:2965: TypeError
___________________________ test_thresholded_scorers ___________________________

    def test_thresholded_scorers():
        # Test scorers that take thresholds.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LogisticRegression(random_state=0)
        clf.fit(X_train, y_train)
        score1 = get_scorer("roc_auc")(clf, X_test, y_test)
        score2 = roc_auc_score(y_test, clf.decision_function(X_test))
        score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
>       assert_almost_equal(score1, score2)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(0.9230769230769231), np.float64(1.0)), kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(0.9230769230769231)
E            DESIRED: np.float64(1.0)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
______________ test_thresholded_scorers_multilabel_indicator_data ______________

    def test_thresholded_scorers_multilabel_indicator_data():
        # Test that the scorer work with multilabel-indicator format
        # for multilabel and multi-output multi-class classifier
        X, y = make_multilabel_classification(allow_unlabeled=False, random_state=0)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
        # Multi-output multi-class predict_proba
        clf = DecisionTreeClassifier()
        clf.fit(X_train, y_train)
        y_proba = clf.predict_proba(X_test)
        score1 = get_scorer("roc_auc")(clf, X_test, y_test)
        score2 = roc_auc_score(y_test, np.vstack([p[:, -1] for p in y_proba]).T)
        assert_almost_equal(score1, score2)
    
        # Multilabel predict_proba
        clf = OneVsRestClassifier(DecisionTreeClassifier())
        clf.fit(X_train, y_train)
        score1 = get_scorer("roc_auc")(clf, X_test, y_test)
        score2 = roc_auc_score(y_test, clf.predict_proba(X_test))
        assert_almost_equal(score1, score2)
    
        # Multilabel decision function
        clf = OneVsRestClassifier(LinearSVC(random_state=0))
        clf.fit(X_train, y_train)
        score1 = get_scorer("roc_auc")(clf, X_test, y_test)
        score2 = roc_auc_score(y_test, clf.decision_function(X_test))
>       assert_almost_equal(score1, score2)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:540: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (np.float64(0.7575496725496725), np.float64(0.8332766677766678))
kwds = {}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Arrays are not almost equal to 7 decimals
E            ACTUAL: np.float64(0.7575496725496725)
E            DESIRED: np.float64(0.8332766677766678)

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
_______________________ test_supervised_cluster_scorers ________________________

    def test_supervised_cluster_scorers():
        # Test clustering scorers against gold standard labeling.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        km = KMeans(n_clusters=3, n_init="auto")
        km.fit(X_train)
        for name in CLUSTER_SCORERS:
>           score1 = get_scorer(name)(km, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:550: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 1, 1])
labels_pred = array([0, 0, 2, 1, 2, 0, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2,
       2, 0, 0], dtype=int32)

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
__________________________ test_raises_on_score_list ___________________________

    def test_raises_on_score_list():
        # Test that when a list of scores is returned, we raise proper errors.
        X, y = make_blobs(random_state=0)
        f1_scorer_no_average = make_scorer(f1_score, average=None)
        clf = DecisionTreeClassifier()
        with pytest.raises(ValueError):
            cross_val_score(clf, X, y, scoring=f1_scorer_no_average)
        grid_search = GridSearchCV(
            clf, scoring=f1_scorer_no_average, param_grid={"max_depth": [1, 2]}
        )
        with pytest.raises(ValueError):
>           grid_search.fit(X, y)
E           Failed: DID NOT RAISE <class 'ValueError'>

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:566: Failed
________________ test_scorer_memmap_input[adjusted_rand_score] _________________

name = 'adjusted_rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
_____________________ test_scorer_memmap_input[f1_samples] _____________________

name = 'f1_samples'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
>               score = scorer(estimator, X_mm, y_ml_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:691: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1289: in f1_score
    return fbeta_score(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1482: in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
y_pred = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
__________________ test_scorer_memmap_input[jaccard_samples] ___________________

name = 'jaccard_samples'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
>               score = scorer(estimator, X_mm, y_ml_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:691: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:888: in jaccard_score
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
y_pred = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_________________ test_scorer_memmap_input[precision_samples] __________________

name = 'precision_samples'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
>               score = scorer(estimator, X_mm, y_ml_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:691: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2209: in precision_score
    p, _, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
y_pred = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
_____________________ test_scorer_memmap_input[rand_score] _____________________

name = 'rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:259: in rand_score
    contingency = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
___________________ test_scorer_memmap_input[recall_samples] ___________________

name = 'recall_samples'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
>               score = scorer(estimator, X_mm, y_ml_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:691: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2391: in recall_score
    _, r, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
y_pred = <30x5 sparse matrix of type '<class 'numpy.int64'>'
	with 70 stored elements in Compressed Sparse Row format>
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
                        raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
                labels = [pos_label]
            else:
                average_options = list(average_options)
                if y_type == "multiclass":
                    average_options.remove("samples")
>               raise ValueError(
                    "Target is %s but average='binary'. Please "
                    "choose another average setting, one of %r." % (y_type, average_options)
E                   ValueError: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1576: ValueError
____________________________ test_deprecated_scorer ____________________________

    def test_deprecated_scorer():
        X, y = make_regression(n_samples=10, n_features=1, random_state=0)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        reg = DecisionTreeRegressor()
        reg.fit(X_train, y_train)
        deprecated_scorer = get_scorer("max_error")
        with pytest.warns(DeprecationWarning):
>           deprecated_scorer(reg, X_test, y_test)
E           Failed: DID NOT WARN. No warnings of type (<class 'DeprecationWarning'>,) were emitted.
E            Emitted warnings: [].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:717: Failed
__________ test_multimetric_scorer_calls_method_once[scorers0-1-1-1] ___________

scorers = {'a1': 'accuracy', 'a2': 'accuracy', 'll1': 'neg_log_loss', 'll2': 'neg_log_loss', ...}
expected_predict_count = 1, expected_predict_proba_count = 1
expected_decision_func_count = 1

    @pytest.mark.parametrize(
        (
            "scorers,expected_predict_count,"
            "expected_predict_proba_count,expected_decision_func_count"
        ),
        [
            (
                {
                    "a1": "accuracy",
                    "a2": "accuracy",
                    "ll1": "neg_log_loss",
                    "ll2": "neg_log_loss",
                    "ra1": "roc_auc",
                    "ra2": "roc_auc",
                },
                1,
                1,
                1,
            ),
            (["roc_auc", "accuracy"], 1, 0, 1),
            (["neg_log_loss", "accuracy"], 1, 1, 0),
        ],
    )
    def test_multimetric_scorer_calls_method_once(
        scorers,
        expected_predict_count,
        expected_predict_proba_count,
        expected_decision_func_count,
    ):
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
        pos_proba = np.random.rand(X.shape[0])
        proba = np.c_[1 - pos_proba, pos_proba]
    
        class MyClassifier(ClassifierMixin, BaseEstimator):
            def __init__(self):
                self._expected_predict_count = 0
                self._expected_predict_proba_count = 0
                self._expected_decision_function_count = 0
    
            def fit(self, X, y):
                self.classes_ = np.unique(y)
                return self
    
            def predict(self, X):
                self._expected_predict_count += 1
                return y
    
            def predict_proba(self, X):
                self._expected_predict_proba_count += 1
                return proba
    
            def decision_function(self, X):
                self._expected_decision_function_count += 1
                return pos_proba
    
        mock_est = MyClassifier().fit(X, y)
        scorer_dict = _check_multimetric_scoring(LogisticRegression(), scorers)
        multi_scorer = _MultimetricScorer(scorers=scorer_dict)
        results = multi_scorer(mock_est, X, y)
    
        assert set(scorers) == set(results)  # compare dict keys
    
>       assert mock_est._expected_predict_count == expected_predict_count
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:782: AssertionError
__________ test_multimetric_scorer_calls_method_once[scorers1-1-0-1] ___________

scorers = ['roc_auc', 'accuracy'], expected_predict_count = 1
expected_predict_proba_count = 0, expected_decision_func_count = 1

    @pytest.mark.parametrize(
        (
            "scorers,expected_predict_count,"
            "expected_predict_proba_count,expected_decision_func_count"
        ),
        [
            (
                {
                    "a1": "accuracy",
                    "a2": "accuracy",
                    "ll1": "neg_log_loss",
                    "ll2": "neg_log_loss",
                    "ra1": "roc_auc",
                    "ra2": "roc_auc",
                },
                1,
                1,
                1,
            ),
            (["roc_auc", "accuracy"], 1, 0, 1),
            (["neg_log_loss", "accuracy"], 1, 1, 0),
        ],
    )
    def test_multimetric_scorer_calls_method_once(
        scorers,
        expected_predict_count,
        expected_predict_proba_count,
        expected_decision_func_count,
    ):
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
        pos_proba = np.random.rand(X.shape[0])
        proba = np.c_[1 - pos_proba, pos_proba]
    
        class MyClassifier(ClassifierMixin, BaseEstimator):
            def __init__(self):
                self._expected_predict_count = 0
                self._expected_predict_proba_count = 0
                self._expected_decision_function_count = 0
    
            def fit(self, X, y):
                self.classes_ = np.unique(y)
                return self
    
            def predict(self, X):
                self._expected_predict_count += 1
                return y
    
            def predict_proba(self, X):
                self._expected_predict_proba_count += 1
                return proba
    
            def decision_function(self, X):
                self._expected_decision_function_count += 1
                return pos_proba
    
        mock_est = MyClassifier().fit(X, y)
        scorer_dict = _check_multimetric_scoring(LogisticRegression(), scorers)
        multi_scorer = _MultimetricScorer(scorers=scorer_dict)
        results = multi_scorer(mock_est, X, y)
    
        assert set(scorers) == set(results)  # compare dict keys
    
>       assert mock_est._expected_predict_count == expected_predict_count
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:782: AssertionError
__________ test_multimetric_scorer_calls_method_once[scorers2-1-1-0] ___________

scorers = ['neg_log_loss', 'accuracy'], expected_predict_count = 1
expected_predict_proba_count = 1, expected_decision_func_count = 0

    @pytest.mark.parametrize(
        (
            "scorers,expected_predict_count,"
            "expected_predict_proba_count,expected_decision_func_count"
        ),
        [
            (
                {
                    "a1": "accuracy",
                    "a2": "accuracy",
                    "ll1": "neg_log_loss",
                    "ll2": "neg_log_loss",
                    "ra1": "roc_auc",
                    "ra2": "roc_auc",
                },
                1,
                1,
                1,
            ),
            (["roc_auc", "accuracy"], 1, 0, 1),
            (["neg_log_loss", "accuracy"], 1, 1, 0),
        ],
    )
    def test_multimetric_scorer_calls_method_once(
        scorers,
        expected_predict_count,
        expected_predict_proba_count,
        expected_decision_func_count,
    ):
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
        pos_proba = np.random.rand(X.shape[0])
        proba = np.c_[1 - pos_proba, pos_proba]
    
        class MyClassifier(ClassifierMixin, BaseEstimator):
            def __init__(self):
                self._expected_predict_count = 0
                self._expected_predict_proba_count = 0
                self._expected_decision_function_count = 0
    
            def fit(self, X, y):
                self.classes_ = np.unique(y)
                return self
    
            def predict(self, X):
                self._expected_predict_count += 1
                return y
    
            def predict_proba(self, X):
                self._expected_predict_proba_count += 1
                return proba
    
            def decision_function(self, X):
                self._expected_decision_function_count += 1
                return pos_proba
    
        mock_est = MyClassifier().fit(X, y)
        scorer_dict = _check_multimetric_scoring(LogisticRegression(), scorers)
        multi_scorer = _MultimetricScorer(scorers=scorer_dict)
        results = multi_scorer(mock_est, X, y)
    
        assert set(scorers) == set(results)  # compare dict keys
    
>       assert mock_est._expected_predict_count == expected_predict_count
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:782: AssertionError
__ test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0] __

scorers = ['roc_auc', 'neg_log_loss']

    @pytest.mark.parametrize(
        "scorers",
        [
            (["roc_auc", "neg_log_loss"]),
            (
                {
                    "roc_auc": make_scorer(
                        roc_auc_score,
                        response_method=["predict_proba", "decision_function"],
                    ),
                    "neg_log_loss": make_scorer(log_loss, response_method="predict_proba"),
                }
            ),
        ],
    )
    def test_multimetric_scorer_calls_method_once_classifier_no_decision(scorers):
        predict_proba_call_cnt = 0
    
        class MockKNeighborsClassifier(KNeighborsClassifier):
            def predict_proba(self, X):
                nonlocal predict_proba_call_cnt
                predict_proba_call_cnt += 1
                return super().predict_proba(X)
    
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
    
        # no decision function
        clf = MockKNeighborsClassifier(n_neighbors=1)
        clf.fit(X, y)
    
        scorer_dict = _check_multimetric_scoring(clf, scorers)
        scorer = _MultimetricScorer(scorers=scorer_dict)
        scorer(clf, X, y)
    
>       assert predict_proba_call_cnt == 1
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:821: AssertionError
__ test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1] __

scorers = {'neg_log_loss': <sklearn.metrics.temp.Scorer object at 0x70c88738eee0>, 'roc_auc': <sklearn.metrics.temp.Scorer object at 0x70c88738efa0>}

    @pytest.mark.parametrize(
        "scorers",
        [
            (["roc_auc", "neg_log_loss"]),
            (
                {
                    "roc_auc": make_scorer(
                        roc_auc_score,
                        response_method=["predict_proba", "decision_function"],
                    ),
                    "neg_log_loss": make_scorer(log_loss, response_method="predict_proba"),
                }
            ),
        ],
    )
    def test_multimetric_scorer_calls_method_once_classifier_no_decision(scorers):
        predict_proba_call_cnt = 0
    
        class MockKNeighborsClassifier(KNeighborsClassifier):
            def predict_proba(self, X):
                nonlocal predict_proba_call_cnt
                predict_proba_call_cnt += 1
                return super().predict_proba(X)
    
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
    
        # no decision function
        clf = MockKNeighborsClassifier(n_neighbors=1)
        clf.fit(X, y)
    
>       scorer_dict = _check_multimetric_scoring(clf, scorers)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:817: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:430: in _check_multimetric_scoring
    scorers = {key: check_scoring(estimator, scoring=scorer) for key, scorer in scoring.items()}
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:430: in <dictcomp>
    scorers = {key: check_scoring(estimator, scoring=scorer) for key, scorer in scoring.items()}
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MockKNeighborsClassifier(n_neighbors=1)
scoring = <sklearn.metrics.temp.Scorer object at 0x70c88738efa0>

    @validate_params({'estimator': [HasMethods('fit'), None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, set, tuple, dict, None], 'allow_none': ['boolean'], 'raise_exc': ['boolean']}, prefer_skip_nested_validation=True)
    def check_scoring(estimator=None, scoring=None, *, allow_none=False, raise_exc=True):
        """Determine scorer from user options.
    
        A TypeError will be thrown if the estimator cannot be scored.
    
        Parameters
        ----------
        estimator : estimator object implementing 'fit' or None, default=None
            The object to use to fit the data. If `None`, then this function may error
            depending on `allow_none`.
    
        scoring : str, callable, list, tuple, set, or dict, default=None
            Scorer to use. If `scoring` represents a single score, one can use:
    
            - a single string (see :ref:`scoring_parameter`);
            - a callable (see :ref:`scoring`) that returns a single value.
    
            If `scoring` represents multiple scores, one can use:
    
            - a list, tuple or set of unique strings;
            - a callable returning a dictionary where the keys are the metric names and the
              values are the metric scorers;
            - a dictionary with metric names as keys and callables a values. The callables
              need to have the signature `callable(estimator, X, y)`.
    
            If None, the provided estimator object's `score` method is used.
    
        allow_none : bool, default=False
            Whether to return None or raise an error if no `scoring` is specified and the
            estimator has no `score` method.
    
        raise_exc : bool, default=True
            Whether to raise an exception (if a subset of the scorers in multimetric scoring
            fails) or to return an error code.
    
            - If set to `True`, raises the failing scorer's exception.
            - If set to `False`, a formatted string of the exception details is passed as
              result of the failing scorer(s).
    
            This applies if `scoring` is list, tuple, set, or dict. Ignored if `scoring` is
            a str or a callable.
    
            .. versionadded:: 1.6
    
        Returns
        -------
        scoring : callable
            A scorer callable object / function with signature ``scorer(estimator, X, y)``.
    
        Examples
        --------
        >>> from sklearn.datasets import load_iris
        >>> from sklearn.metrics import check_scoring
        >>> from sklearn.tree import DecisionTreeClassifier
        >>> X, y = load_iris(return_X_y=True)
        >>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)
        >>> scorer = check_scoring(classifier, scoring='accuracy')
        >>> scorer(classifier, X, y)
        0.96...
    
        >>> from sklearn.metrics import make_scorer, accuracy_score, mean_squared_log_error
        >>> X, y = load_iris(return_X_y=True)
        >>> y *= -1
        >>> clf = DecisionTreeClassifier().fit(X, y)
        >>> scoring = {
        ...     "accuracy": make_scorer(accuracy_score),
        ...     "mean_squared_log_error": make_scorer(mean_squared_log_error),
        ... }
        >>> scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=False)
        >>> scores = scoring_call(clf, X, y)
        >>> scores
        {'accuracy': 1.0, 'mean_squared_log_error': 'Traceback ...'}
        """
        if isinstance(scoring, str):
            return get_scorer(scoring)
        if callable(scoring):
            module = getattr(scoring, '__module__', None)
            if hasattr(module, 'startswith') and module.startswith('sklearn.metrics.') and (not module.startswith('sklearn.metrics._scorer')) and (not module.startswith('sklearn.metrics.tests.')):
>               raise ValueError('scoring value %r looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.' % scoring)
E               ValueError: scoring value <sklearn.metrics.temp.Scorer object at 0x70c88738efa0> looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:672: ValueError
________ test_multimetric_scorer_calls_method_once_regressor_threshold _________

    def test_multimetric_scorer_calls_method_once_regressor_threshold():
        predict_called_cnt = 0
    
        class MockDecisionTreeRegressor(DecisionTreeRegressor):
            def predict(self, X):
                nonlocal predict_called_cnt
                predict_called_cnt += 1
                return super().predict(X)
    
        X, y = np.array([[1], [1], [0], [0], [0]]), np.array([0, 1, 1, 1, 0])
    
        # no decision function
        clf = MockDecisionTreeRegressor()
        clf.fit(X, y)
    
        scorers = {"neg_mse": "neg_mean_squared_error", "r2": "r2"}
        scorer_dict = _check_multimetric_scoring(clf, scorers)
        scorer = _MultimetricScorer(scorers=scorer_dict)
        scorer(clf, X, y)
    
>       assert predict_called_cnt == 1
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:844: AssertionError
____________ test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0] _____________

scorer_name = 'roc_auc_ovr'
metric = functools.partial(<function roc_auc_score at 0x70c891ae93a0>, multi_class='ovr')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("roc_auc_ovr", partial(roc_auc_score, multi_class="ovr")),
            ("roc_auc_ovo", partial(roc_auc_score, multi_class="ovo")),
            (
                "roc_auc_ovr_weighted",
                partial(roc_auc_score, multi_class="ovr", average="weighted"),
            ),
            (
                "roc_auc_ovo_weighted",
                partial(roc_auc_score, multi_class="ovo", average="weighted"),
            ),
        ],
    )
    def test_multiclass_roc_proba_scorer(scorer_name, metric):
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
        y_proba = lr.predict_proba(X)
        expected_score = metric(y, y_proba)
    
>       assert scorer(lr, X, y) == pytest.approx(expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:946: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
____________ test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1] _____________

scorer_name = 'roc_auc_ovo'
metric = functools.partial(<function roc_auc_score at 0x70c891ae93a0>, multi_class='ovo')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("roc_auc_ovr", partial(roc_auc_score, multi_class="ovr")),
            ("roc_auc_ovo", partial(roc_auc_score, multi_class="ovo")),
            (
                "roc_auc_ovr_weighted",
                partial(roc_auc_score, multi_class="ovr", average="weighted"),
            ),
            (
                "roc_auc_ovo_weighted",
                partial(roc_auc_score, multi_class="ovo", average="weighted"),
            ),
        ],
    )
    def test_multiclass_roc_proba_scorer(scorer_name, metric):
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
        y_proba = lr.predict_proba(X)
        expected_score = metric(y, y_proba)
    
>       assert scorer(lr, X, y) == pytest.approx(expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:946: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
________ test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2] ________

scorer_name = 'roc_auc_ovr_weighted'
metric = functools.partial(<function roc_auc_score at 0x70c891ae93a0>, multi_class='ovr', average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("roc_auc_ovr", partial(roc_auc_score, multi_class="ovr")),
            ("roc_auc_ovo", partial(roc_auc_score, multi_class="ovo")),
            (
                "roc_auc_ovr_weighted",
                partial(roc_auc_score, multi_class="ovr", average="weighted"),
            ),
            (
                "roc_auc_ovo_weighted",
                partial(roc_auc_score, multi_class="ovo", average="weighted"),
            ),
        ],
    )
    def test_multiclass_roc_proba_scorer(scorer_name, metric):
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
        y_proba = lr.predict_proba(X)
        expected_score = metric(y, y_proba)
    
>       assert scorer(lr, X, y) == pytest.approx(expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:946: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
________ test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3] ________

scorer_name = 'roc_auc_ovo_weighted'
metric = functools.partial(<function roc_auc_score at 0x70c891ae93a0>, multi_class='ovo', average='weighted')

    @pytest.mark.parametrize(
        "scorer_name, metric",
        [
            ("roc_auc_ovr", partial(roc_auc_score, multi_class="ovr")),
            ("roc_auc_ovo", partial(roc_auc_score, multi_class="ovo")),
            (
                "roc_auc_ovr_weighted",
                partial(roc_auc_score, multi_class="ovr", average="weighted"),
            ),
            (
                "roc_auc_ovo_weighted",
                partial(roc_auc_score, multi_class="ovo", average="weighted"),
            ),
        ],
    )
    def test_multiclass_roc_proba_scorer(scorer_name, metric):
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
        y_proba = lr.predict_proba(X)
        expected_score = metric(y, y_proba)
    
>       assert scorer(lr, X, y) == pytest.approx(expected_score)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:946: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
____________________ test_multiclass_roc_proba_scorer_label ____________________

    def test_multiclass_roc_proba_scorer_label():
        scorer = make_scorer(
            roc_auc_score,
            multi_class="ovo",
            labels=[0, 1, 2],
            response_method="predict_proba",
        )
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
        y_proba = lr.predict_proba(X)
    
        y_binary = y == 0
        expected_score = roc_auc_score(
            y_binary, y_proba, multi_class="ovo", labels=[0, 1, 2]
        )
    
>       assert scorer(lr, X, y_binary) == pytest.approx(expected_score)
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:967: AssertionError
___________ test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr] ____________

scorer_name = 'roc_auc_ovr'

    @pytest.mark.parametrize(
        "scorer_name",
        ["roc_auc_ovr", "roc_auc_ovo", "roc_auc_ovr_weighted", "roc_auc_ovo_weighted"],
    )
    def test_multiclass_roc_no_proba_scorer_errors(scorer_name):
        # Perceptron has no predict_proba
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = Perceptron().fit(X, y)
        msg = "Perceptron has none of the following attributes: predict_proba."
        with pytest.raises(AttributeError, match=msg):
>           scorer(lr, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:983: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
___________ test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo] ____________

scorer_name = 'roc_auc_ovo'

    @pytest.mark.parametrize(
        "scorer_name",
        ["roc_auc_ovr", "roc_auc_ovo", "roc_auc_ovr_weighted", "roc_auc_ovo_weighted"],
    )
    def test_multiclass_roc_no_proba_scorer_errors(scorer_name):
        # Perceptron has no predict_proba
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = Perceptron().fit(X, y)
        msg = "Perceptron has none of the following attributes: predict_proba."
        with pytest.raises(AttributeError, match=msg):
>           scorer(lr, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:983: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
_______ test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted] _______

scorer_name = 'roc_auc_ovr_weighted'

    @pytest.mark.parametrize(
        "scorer_name",
        ["roc_auc_ovr", "roc_auc_ovo", "roc_auc_ovr_weighted", "roc_auc_ovo_weighted"],
    )
    def test_multiclass_roc_no_proba_scorer_errors(scorer_name):
        # Perceptron has no predict_proba
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = Perceptron().fit(X, y)
        msg = "Perceptron has none of the following attributes: predict_proba."
        with pytest.raises(AttributeError, match=msg):
>           scorer(lr, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:983: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
_______ test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted] _______

scorer_name = 'roc_auc_ovo_weighted'

    @pytest.mark.parametrize(
        "scorer_name",
        ["roc_auc_ovr", "roc_auc_ovo", "roc_auc_ovr_weighted", "roc_auc_ovo_weighted"],
    )
    def test_multiclass_roc_no_proba_scorer_errors(scorer_name):
        # Perceptron has no predict_proba
        scorer = get_scorer(scorer_name)
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = Perceptron().fit(X, y)
        msg = "Perceptron has none of the following attributes: predict_proba."
        with pytest.raises(AttributeError, match=msg):
>           scorer(lr, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:983: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])
y_score = array([2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2])

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "macro", "samples", "weighted"}), None],
            "sample_weight": ["array-like", None],
            "max_fpr": [Interval(Real, 0.0, 1, closed="right"), None],
            "multi_class": [StrOptions({"raise", "ovr", "ovo"})],
            "labels": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def roc_auc_score(
        y_true,
        y_score,
        *,
        average="macro",
        sample_weight=None,
        max_fpr=None,
        multi_class="raise",
        labels=None,
    ):
        """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \
        from prediction scores.
    
        Note: this implementation can be used with binary, multiclass and
        multilabel classification, but some restrictions apply (see Parameters).
    
        Read more in the :ref:`User Guide <roc_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True labels or binary label indicators. The binary and multiclass cases
            expect labels with shape (n_samples,) while the multilabel case expects
            binary label indicators with shape (n_samples, n_classes).
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores.
    
            * In the binary case, it corresponds to an array of shape
              `(n_samples,)`. Both probability estimates and non-thresholded
              decision values can be provided. The probability estimates correspond
              to the **probability of the class with the greater label**,
              i.e. `estimator.classes_[1]` and thus
              `estimator.predict_proba(X, y)[:, 1]`. The decision values
              corresponds to the output of `estimator.decision_function(X, y)`.
              See more information in the :ref:`User guide <roc_auc_binary>`;
            * In the multiclass case, it corresponds to an array of shape
              `(n_samples, n_classes)` of probability estimates provided by the
              `predict_proba` method. The probability estimates **must**
              sum to 1 across the possible classes. In addition, the order of the
              class scores must correspond to the order of ``labels``,
              if provided, or else to the numerical or lexicographical order of
              the labels in ``y_true``. See more information in the
              :ref:`User guide <roc_auc_multiclass>`;
            * In the multilabel case, it corresponds to an array of shape
              `(n_samples, n_classes)`. Probability estimates are provided by the
              `predict_proba` method and the non-thresholded decision values by
              the `decision_function` method. The probability estimates correspond
              to the **probability of the class with the greater label for each
              output** of the classifier. See more information in the
              :ref:`User guide <roc_auc_multilabel>`.
    
        average : {'micro', 'macro', 'samples', 'weighted'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned.
            Otherwise, this determines the type of averaging performed on the data.
            Note: multiclass ROC AUC currently only handles the 'macro' and
            'weighted' averages. For multiclass targets, `average=None` is only
            implemented for `multi_class='ovr'` and `average='micro'` is only
            implemented for `multi_class='ovr'`.
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        max_fpr : float > 0 and <= 1, default=None
            If not ``None``, the standardized partial AUC [2]_ over the range
            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
            should be either equal to ``None`` or ``1.0`` as AUC ROC partial
            computation currently is not supported for multiclass.
    
        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
            Only used for multiclass targets. Determines the type of configuration
            to use. The default value raises an error, so either
            ``'ovr'`` or ``'ovo'`` must be passed explicitly.
    
            ``'ovr'``:
                Stands for One-vs-rest. Computes the AUC of each class
                against the rest [3]_ [4]_. This
                treats the multiclass case in the same way as the multilabel case.
                Sensitive to class imbalance even when ``average == 'macro'``,
                because class imbalance affects the composition of each of the
                'rest' groupings.
            ``'ovo'``:
                Stands for One-vs-one. Computes the average AUC of all
                possible pairwise combinations of classes [5]_.
                Insensitive to class imbalance when
                ``average == 'macro'``.
    
        labels : array-like of shape (n_classes,), default=None
            Only used for multiclass targets. List of labels that index the
            classes in ``y_score``. If ``None``, the numerical or lexicographical
            order of the labels in ``y_true`` is used.
    
        Returns
        -------
        auc : float
            Area Under the Curve score.
    
        See Also
        --------
        average_precision_score : Area under the precision-recall curve.
        roc_curve : Compute Receiver operating characteristic (ROC) curve.
        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
            (ROC) curve given an estimator and some data.
        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
            (ROC) curve given the true and predicted values.
    
        Notes
        -----
        The Gini Coefficient is a summary measure of the ranking ability of binary
        classifiers. It is expressed using the area under of the ROC as follows:
    
        G = 2 * AUC - 1
    
        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
        will ensure that random guessing will yield a score of 0 in expectation, and it is
        upper bounded by 1.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Receiver operating characteristic
                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_
    
        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989
                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_
    
        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
               probability estimation trees (Section 6.2), CeDER Working Paper
               #IS-00-04, Stern School of Business, New York University.
    
        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
                Recognition Letters, 27(8), 861-874.
                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_
    
        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
                Under the ROC Curve for Multiple Class Classification Problems.
                Machine Learning, 45(2), 171-186.
                <http://link.springer.com/article/10.1023/A:1010920819831>`_
        .. [6] `Wikipedia entry for the Gini coefficient
                <https://en.wikipedia.org/wiki/Gini_coefficient>`_
    
        Examples
        --------
        Binary case:
    
        >>> from sklearn.datasets import load_breast_cancer
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.metrics import roc_auc_score
        >>> X, y = load_breast_cancer(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
        np.float64(0.99...)
        >>> roc_auc_score(y, clf.decision_function(X))
        np.float64(0.99...)
    
        Multiclass case:
    
        >>> from sklearn.datasets import load_iris
        >>> X, y = load_iris(return_X_y=True)
        >>> clf = LogisticRegression(solver="liblinear").fit(X, y)
        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
        np.float64(0.99...)
    
        Multilabel case:
    
        >>> import numpy as np
        >>> from sklearn.datasets import make_multilabel_classification
        >>> from sklearn.multioutput import MultiOutputClassifier
        >>> X, y = make_multilabel_classification(random_state=0)
        >>> clf = MultiOutputClassifier(clf).fit(X, y)
        >>> # get a list of n_output containing probability arrays of shape
        >>> # (n_samples, n_classes)
        >>> y_pred = clf.predict_proba(X)
        >>> # extract the positive columns for each output
        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
        >>> roc_auc_score(y, y_pred, average=None)
        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
        >>> from sklearn.linear_model import RidgeClassifierCV
        >>> clf = RidgeClassifierCV().fit(X, y)
        >>> roc_auc_score(y, clf.decision_function(X), average=None)
        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
        """
    
        y_type = type_of_target(y_true, input_name="y_true")
        y_true = check_array(y_true, ensure_2d=False, dtype=None)
        y_score = check_array(y_score, ensure_2d=False)
    
        if y_type == "multiclass" or (
            y_type == "binary" and y_score.ndim == 2 and y_score.shape[1] > 2
        ):
            # do not support partial ROC computation for multiclass
            if max_fpr is not None and max_fpr != 1.0:
                raise ValueError(
                    "Partial AUC computation not available in "
                    "multiclass setting, 'max_fpr' must be"
                    " set to `None`, received `max_fpr={0}` "
                    "instead".format(max_fpr)
                )
            if multi_class == "raise":
>               raise ValueError("multi_class must be in ('ovo', 'ovr')")
E               ValueError: multi_class must be in ('ovo', 'ovr')

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:630: ValueError
_______________________ test_average_precision_pos_label _______________________

string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    def test_average_precision_pos_label(string_labeled_classification_problem):
        # check that _Scorer will lead to the right score when passing
        # `pos_label`. Currently, only `average_precision_score` is defined to
        # be such a scorer.
        (
            clf,
            X_test,
            y_test,
            _,
            y_pred_proba,
            y_pred_decision,
        ) = string_labeled_classification_problem
    
        pos_label = "cancer"
        # we need to select the positive column or reverse the decision values
        y_pred_proba = y_pred_proba[:, 0]
        y_pred_decision = y_pred_decision * -1
        assert clf.classes_[0] == pos_label
    
        # check that when calling the scoring function, probability estimates and
        # decision values lead to the same results
        ap_proba = average_precision_score(y_test, y_pred_proba, pos_label=pos_label)
        ap_decision_function = average_precision_score(
            y_test, y_pred_decision, pos_label=pos_label
        )
        assert ap_proba == pytest.approx(ap_decision_function)
    
        # create a scorer which would require to pass a `pos_label`
        # check that it fails if `pos_label` is not provided
        average_precision_scorer = make_scorer(
            average_precision_score,
            response_method=("decision_function", "predict_proba"),
        )
        err_msg = "pos_label=1 is not a valid label. It should be one of "
        with pytest.raises(ValueError, match=err_msg):
            average_precision_scorer(clf, X_test, y_test)
    
        # otherwise, the scorer should give the same results than calling the
        # scoring function
        average_precision_scorer = make_scorer(
            average_precision_score,
            response_method=("decision_function", "predict_proba"),
            pos_label=pos_label,
        )
>       ap_scorer = average_precision_scorer(clf, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1086: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_score = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_score": ["array-like"],
            "average": [StrOptions({"micro", "samples", "weighted", "macro"}), None],
            "pos_label": [Real, str, "boolean"],
            "sample_weight": ["array-like", None],
        },
        prefer_skip_nested_validation=True,
    )
    def average_precision_score(
        y_true, y_score, *, average="macro", pos_label=1, sample_weight=None
    ):
        """Compute average precision (AP) from prediction scores.
    
        AP summarizes a precision-recall curve as the weighted mean of precisions
        achieved at each threshold, with the increase in recall from the previous
        threshold used as the weight:
    
        .. math::
            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n
    
        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth
        threshold [1]_. This implementation is not interpolated and is different
        from computing the area under the precision-recall curve with the
        trapezoidal rule, which uses linear interpolation and can be too
        optimistic.
    
        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
            True binary labels or binary label indicators.
    
        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
            Target scores, can either be probability estimates of the positive
            class, confidence values, or non-thresholded measure of decisions
            (as returned by :term:`decision_function` on some classifiers).
    
        average : {'micro', 'samples', 'weighted', 'macro'} or None, \
                default='macro'
            If ``None``, the scores for each class are returned. Otherwise,
            this determines the type of averaging performed on the data:
    
            ``'micro'``:
                Calculate metrics globally by considering each element of the label
                indicator matrix as a label.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average, weighted
                by support (the number of true instances for each label).
            ``'samples'``:
                Calculate metrics for each instance, and find their average.
    
            Will be ignored when ``y_true`` is binary.
    
        pos_label : int, float, bool or str, default=1
            The label of the positive class. Only applied to binary ``y_true``.
            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        Returns
        -------
        average_precision : float
            Average precision score.
    
        See Also
        --------
        roc_auc_score : Compute the area under the ROC curve.
        precision_recall_curve : Compute precision-recall pairs for different
            probability thresholds.
    
        Notes
        -----
        .. versionchanged:: 0.19
          Instead of linearly interpolating between operating points, precisions
          are weighted by the change in recall since the last operating point.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Average precision
               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&
               oldid=793358396#Average_precision>`_
    
        Examples
        --------
        >>> import numpy as np
        >>> from sklearn.metrics import average_precision_score
        >>> y_true = np.array([0, 0, 1, 1])
        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
        >>> average_precision_score(y_true, y_scores)
        np.float64(0.83...)
        >>> y_true = np.array([0, 0, 1, 1, 2, 2])
        >>> y_scores = np.array([
        ...     [0.7, 0.2, 0.1],
        ...     [0.4, 0.3, 0.3],
        ...     [0.1, 0.8, 0.1],
        ...     [0.2, 0.3, 0.5],
        ...     [0.4, 0.4, 0.2],
        ...     [0.1, 0.2, 0.7],
        ... ])
        >>> average_precision_score(y_true, y_scores)
        np.float64(0.77...)
        """
    
        def _binary_uninterpolated_average_precision(
            y_true, y_score, pos_label=1, sample_weight=None
        ):
            precision, recall, _ = precision_recall_curve(
                y_true, y_score, pos_label=pos_label, sample_weight=sample_weight
            )
            # Return the step function integral
            # The following works because the last entry of precision is
            # guaranteed to be 1, as returned by precision_recall_curve.
            # Due to numerical error, we can get `-0.0` and we therefore clip it.
            return max(0.0, -np.sum(np.diff(recall) * np.array(precision)[:-1]))
    
        y_type = type_of_target(y_true, input_name="y_true")
    
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = np.unique(y_true).tolist()
    
        if y_type == "binary":
            if len(present_labels) == 2 and pos_label not in present_labels:
>               raise ValueError(
                    f"pos_label={pos_label} is not a valid label. It should be "
                    f"one of {present_labels}"
                )
E               ValueError: pos_label=1 is not a valid label. It should be one of ['cancer', 'not cancer']

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_ranking.py:239: ValueError
_______________________ test_brier_score_loss_pos_label ________________________

string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    def test_brier_score_loss_pos_label(string_labeled_classification_problem):
        # check that _Scorer leads to the right score when `pos_label` is
        # provided. Currently only the `brier_score_loss` is defined to be such
        # a scorer.
        clf, X_test, y_test, _, y_pred_proba, _ = string_labeled_classification_problem
    
        pos_label = "cancer"
        assert clf.classes_[0] == pos_label
    
        # brier score loss is symmetric
        brier_pos_cancer = brier_score_loss(y_test, y_pred_proba[:, 0], pos_label="cancer")
        brier_pos_not_cancer = brier_score_loss(
            y_test, y_pred_proba[:, 1], pos_label="not cancer"
        )
        assert brier_pos_cancer == pytest.approx(brier_pos_not_cancer)
    
        brier_scorer = make_scorer(
            brier_score_loss,
            response_method="predict_proba",
            pos_label=pos_label,
        )
>       assert brier_scorer(clf, X_test, y_test) == pytest.approx(brier_pos_cancer)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_proba = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)

    @validate_params(
        {
            "y_true": ["array-like"],
            "y_proba": ["array-like", Hidden(None)],
            "sample_weight": ["array-like", None],
            "pos_label": [Real, str, "boolean", None],
            "y_prob": ["array-like", Hidden(StrOptions({"deprecated"}))],
        },
        prefer_skip_nested_validation=True,
    )
    def brier_score_loss(
        y_true, y_proba=None, *, sample_weight=None, pos_label=None, y_prob="deprecated"
    ):
        """Compute the Brier score loss.
    
        The smaller the Brier score loss, the better, hence the naming with "loss".
        The Brier score measures the mean squared difference between the predicted
        probability and the actual outcome. The Brier score always
        takes on a value between zero and one, since this is the largest
        possible difference between a predicted probability (which must be
        between zero and one) and the actual outcome (which can take on values
        of only 0 and 1). It can be decomposed as the sum of refinement loss and
        calibration loss.
    
        The Brier score is appropriate for binary and categorical outcomes that
        can be structured as true or false, but is inappropriate for ordinal
        variables which can take on three or more values (this is because the
        Brier score assumes that all possible outcomes are equivalently
        "distant" from one another). Which label is considered to be the positive
        label is controlled via the parameter `pos_label`, which defaults to
        the greater label unless `y_true` is all 0 or all -1, in which case
        `pos_label` defaults to 1.
    
        Read more in the :ref:`User Guide <brier_score_loss>`.
    
        Parameters
        ----------
        y_true : array-like of shape (n_samples,)
            True targets.
    
        y_proba : array-like of shape (n_samples,)
            Probabilities of the positive class.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
    
        pos_label : int, float, bool or str, default=None
            Label of the positive class. `pos_label` will be inferred in the
            following manner:
    
            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
            * else if `y_true` contains string, an error will be raised and
              `pos_label` should be explicitly specified;
            * otherwise, `pos_label` defaults to the greater label,
              i.e. `np.unique(y_true)[-1]`.
    
        y_prob : array-like of shape (n_samples,)
            Probabilities of the positive class.
    
            .. deprecated:: 1.5
                `y_prob` is deprecated and will be removed in 1.7. Use
                `y_proba` instead.
    
        Returns
        -------
        score : float
            Brier score loss.
    
        References
        ----------
        .. [1] `Wikipedia entry for the Brier score
                <https://en.wikipedia.org/wiki/Brier_score>`_.
    
        Examples
        --------
        >>> import numpy as np
        >>> from sklearn.metrics import brier_score_loss
        >>> y_true = np.array([0, 1, 1, 0])
        >>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])
        >>> brier_score_loss(y_true, y_prob)
        np.float64(0.037...)
        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
        np.float64(0.037...)
        >>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
        np.float64(0.037...)
        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
        np.float64(0.0)
        """
        # TODO(1.7): remove in 1.7 and reset y_proba to be required
        # Note: validate params will raise an error if y_prob is not array-like,
        # or "deprecated"
        if y_proba is not None and not isinstance(y_prob, str):
            raise ValueError(
                "`y_prob` and `y_proba` cannot be both specified. Please use `y_proba` only"
                " as `y_prob` is deprecated in v1.5 and will be removed in v1.7."
            )
        if y_proba is None:
            warnings.warn(
                (
                    "y_prob was deprecated in version 1.5 and will be removed in 1.7."
                    "Please use ``y_proba`` instead."
                ),
                FutureWarning,
            )
            y_proba = y_prob
    
        y_true = column_or_1d(y_true)
        y_proba = column_or_1d(y_proba)
        assert_all_finite(y_true)
        assert_all_finite(y_proba)
        check_consistent_length(y_true, y_proba, sample_weight)
    
        y_type = type_of_target(y_true, input_name="y_true")
        if y_type != "binary":
            raise ValueError(
                "Only binary classification is supported. The type of the target "
                f"is {y_type}."
            )
    
>       if y_proba.max() > 1:
E       TypeError: '>' not supported between instances of 'str' and 'int'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:3268: TypeError
________________ test_non_symmetric_metric_pos_label[f1_score] _________________

score_func = <function f1_score at 0x70c891af3b80>
string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    @pytest.mark.parametrize(
        "score_func", [f1_score, precision_score, recall_score, jaccard_score]
    )
    def test_non_symmetric_metric_pos_label(
        score_func, string_labeled_classification_problem
    ):
        # check that _Scorer leads to the right score when `pos_label` is
        # provided. We check for all possible metric supported.
        # Note: At some point we may end up having "scorer tags".
        clf, X_test, y_test, y_pred, _, _ = string_labeled_classification_problem
    
        pos_label = "cancer"
        assert clf.classes_[0] == pos_label
    
        score_pos_cancer = score_func(y_test, y_pred, pos_label="cancer")
        score_pos_not_cancer = score_func(y_test, y_pred, pos_label="not cancer")
    
        assert score_pos_cancer != pytest.approx(score_pos_not_cancer)
    
        scorer = make_scorer(score_func, pos_label=pos_label)
>       assert scorer(clf, X_test, y_test) == pytest.approx(score_pos_cancer)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1289: in f1_score
    return fbeta_score(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1482: in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_pred = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
>                       raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
E                       ValueError: pos_label=1 is not a valid label. It should be one of ['cancer', 'not cancer']

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1567: ValueError
_____________ test_non_symmetric_metric_pos_label[precision_score] _____________

score_func = <function precision_score at 0x70c891ae81f0>
string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    @pytest.mark.parametrize(
        "score_func", [f1_score, precision_score, recall_score, jaccard_score]
    )
    def test_non_symmetric_metric_pos_label(
        score_func, string_labeled_classification_problem
    ):
        # check that _Scorer leads to the right score when `pos_label` is
        # provided. We check for all possible metric supported.
        # Note: At some point we may end up having "scorer tags".
        clf, X_test, y_test, y_pred, _, _ = string_labeled_classification_problem
    
        pos_label = "cancer"
        assert clf.classes_[0] == pos_label
    
        score_pos_cancer = score_func(y_test, y_pred, pos_label="cancer")
        score_pos_not_cancer = score_func(y_test, y_pred, pos_label="not cancer")
    
        assert score_pos_cancer != pytest.approx(score_pos_not_cancer)
    
        scorer = make_scorer(score_func, pos_label=pos_label)
>       assert scorer(clf, X_test, y_test) == pytest.approx(score_pos_cancer)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2209: in precision_score
    p, _, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_pred = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
>                       raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
E                       ValueError: pos_label=1 is not a valid label. It should be one of ['cancer', 'not cancer']

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1567: ValueError
______________ test_non_symmetric_metric_pos_label[recall_score] _______________

score_func = <function recall_score at 0x70c891ae8310>
string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    @pytest.mark.parametrize(
        "score_func", [f1_score, precision_score, recall_score, jaccard_score]
    )
    def test_non_symmetric_metric_pos_label(
        score_func, string_labeled_classification_problem
    ):
        # check that _Scorer leads to the right score when `pos_label` is
        # provided. We check for all possible metric supported.
        # Note: At some point we may end up having "scorer tags".
        clf, X_test, y_test, y_pred, _, _ = string_labeled_classification_problem
    
        pos_label = "cancer"
        assert clf.classes_[0] == pos_label
    
        score_pos_cancer = score_func(y_test, y_pred, pos_label="cancer")
        score_pos_not_cancer = score_func(y_test, y_pred, pos_label="not cancer")
    
        assert score_pos_cancer != pytest.approx(score_pos_not_cancer)
    
        scorer = make_scorer(score_func, pos_label=pos_label)
>       assert scorer(clf, X_test, y_test) == pytest.approx(score_pos_cancer)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:2391: in recall_score
    _, r, _, _ = precision_recall_fscore_support(
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1793: in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_pred = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
>                       raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
E                       ValueError: pos_label=1 is not a valid label. It should be one of ['cancer', 'not cancer']

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1567: ValueError
______________ test_non_symmetric_metric_pos_label[jaccard_score] ______________

score_func = <function jaccard_score at 0x70c891af3820>
string_labeled_classification_problem = (LogisticRegression(), array([[21.09, 26.57],
       [20.2 , 26.83],
       [15.05, 19.07],
       [11.8 , 16.58],
   ...08955, 12.88454622,  9.52674548,  4.58937774,
        9.60629185, 10.91750567,  8.86187248,  5.5453814 , 13.44091746]))

    @pytest.mark.parametrize(
        "score_func", [f1_score, precision_score, recall_score, jaccard_score]
    )
    def test_non_symmetric_metric_pos_label(
        score_func, string_labeled_classification_problem
    ):
        # check that _Scorer leads to the right score when `pos_label` is
        # provided. We check for all possible metric supported.
        # Note: At some point we may end up having "scorer tags".
        clf, X_test, y_test, y_pred, _, _ = string_labeled_classification_problem
    
        pos_label = "cancer"
        assert clf.classes_[0] == pos_label
    
        score_pos_cancer = score_func(y_test, y_pred, pos_label="cancer")
        score_pos_not_cancer = score_func(y_test, y_pred, pos_label="not cancer")
    
        assert score_pos_cancer != pytest.approx(score_pos_not_cancer)
    
        scorer = make_scorer(score_func, pos_label=pos_label)
>       assert scorer(clf, X_test, y_test) == pytest.approx(score_pos_cancer)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:41: in __call__
    score = self.score_func(y_true, y_pred, **self.kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:888: in jaccard_score
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

y_true = array(['not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer', 'not c...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
y_pred = array(['not cancer', 'not cancer', 'not cancer', 'cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cance...'not cancer', 'not cancer', 'not cancer', 'not cancer',
       'not cancer', 'not cancer', 'not cancer'], dtype=object)
average = 'binary', labels = None, pos_label = 1

    def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):
        """Validation associated with set-wise metrics.
    
        Returns identified labels.
        """
        average_options = (None, "micro", "macro", "weighted", "samples")
        if average not in average_options and average != "binary":
            raise ValueError("average has to be one of " + str(average_options))
    
        y_true, y_pred = attach_unique(y_true, y_pred)
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        # Convert to Python primitive type to avoid NumPy type / Python str
        # comparison. See https://github.com/numpy/numpy/issues/6784
        present_labels = unique_labels(y_true, y_pred).tolist()
        if average == "binary":
            if y_type == "binary":
                if pos_label not in present_labels:
                    if len(present_labels) >= 2:
>                       raise ValueError(
                            f"pos_label={pos_label} is not a valid label. It "
                            f"should be one of {present_labels}"
                        )
E                       ValueError: pos_label=1 is not a valid label. It should be one of ['cancer', 'not cancer']

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_classification.py:1567: ValueError
____________ test_scorer_select_proba_error[non-thresholded scorer] ____________

scorer = <sklearn.metrics.temp.Scorer object at 0x70c88738e190>

    @pytest.mark.parametrize(
        "scorer",
        [
            make_scorer(
                average_precision_score,
                response_method=("decision_function", "predict_proba"),
                pos_label="xxx",
            ),
            make_scorer(brier_score_loss, response_method="predict_proba", pos_label="xxx"),
            make_scorer(f1_score, pos_label="xxx"),
        ],
        ids=["non-thresholded scorer", "probability scorer", "thresholded scorer"],
    )
    def test_scorer_select_proba_error(scorer):
        # check that we raise the proper error when passing an unknown
        # pos_label
        X, y = make_classification(
            n_classes=2, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
>       assert scorer._kwargs["pos_label"] not in np.unique(y).tolist()
E       AttributeError: 'Scorer' object has no attribute '_kwargs'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1175: AttributeError
______________ test_scorer_select_proba_error[probability scorer] ______________

scorer = <sklearn.metrics.temp.Scorer object at 0x70c88738e1f0>

    @pytest.mark.parametrize(
        "scorer",
        [
            make_scorer(
                average_precision_score,
                response_method=("decision_function", "predict_proba"),
                pos_label="xxx",
            ),
            make_scorer(brier_score_loss, response_method="predict_proba", pos_label="xxx"),
            make_scorer(f1_score, pos_label="xxx"),
        ],
        ids=["non-thresholded scorer", "probability scorer", "thresholded scorer"],
    )
    def test_scorer_select_proba_error(scorer):
        # check that we raise the proper error when passing an unknown
        # pos_label
        X, y = make_classification(
            n_classes=2, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
>       assert scorer._kwargs["pos_label"] not in np.unique(y).tolist()
E       AttributeError: 'Scorer' object has no attribute '_kwargs'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1175: AttributeError
______________ test_scorer_select_proba_error[thresholded scorer] ______________

scorer = <sklearn.metrics.temp.Scorer object at 0x70c88738e220>

    @pytest.mark.parametrize(
        "scorer",
        [
            make_scorer(
                average_precision_score,
                response_method=("decision_function", "predict_proba"),
                pos_label="xxx",
            ),
            make_scorer(brier_score_loss, response_method="predict_proba", pos_label="xxx"),
            make_scorer(f1_score, pos_label="xxx"),
        ],
        ids=["non-thresholded scorer", "probability scorer", "thresholded scorer"],
    )
    def test_scorer_select_proba_error(scorer):
        # check that we raise the proper error when passing an unknown
        # pos_label
        X, y = make_classification(
            n_classes=2, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
>       assert scorer._kwargs["pos_label"] not in np.unique(y).tolist()
E       AttributeError: 'Scorer' object has no attribute '_kwargs'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1175: AttributeError
________________ test_scorer_set_score_request_raises[accuracy] ________________

name = 'accuracy'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______ test_scorer_set_score_request_raises[adjusted_mutual_info_score] _______

name = 'adjusted_mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
__________ test_scorer_set_score_request_raises[adjusted_rand_score] ___________

name = 'adjusted_rand_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[average_precision] ____________

name = 'average_precision'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[balanced_accuracy] ____________

name = 'balanced_accuracy'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[completeness_score] ___________

name = 'completeness_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________ test_scorer_set_score_request_raises[d2_absolute_error_score] _________

name = 'd2_absolute_error_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[explained_variance] ___________

name = 'explained_variance'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________________ test_scorer_set_score_request_raises[f1] ___________________

name = 'f1'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________________ test_scorer_set_score_request_raises[f1_macro] ________________

name = 'f1_macro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________________ test_scorer_set_score_request_raises[f1_micro] ________________

name = 'f1_micro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______________ test_scorer_set_score_request_raises[f1_samples] _______________

name = 'f1_samples'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[f1_weighted] _______________

name = 'f1_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_________ test_scorer_set_score_request_raises[fowlkes_mallows_score] __________

name = 'fowlkes_mallows_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[homogeneity_score] ____________

name = 'homogeneity_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________________ test_scorer_set_score_request_raises[jaccard] _________________

name = 'jaccard'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____________ test_scorer_set_score_request_raises[jaccard_macro] ______________

name = 'jaccard_macro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____________ test_scorer_set_score_request_raises[jaccard_micro] ______________

name = 'jaccard_micro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[jaccard_samples] _____________

name = 'jaccard_samples'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[jaccard_weighted] ____________

name = 'jaccard_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[matthews_corrcoef] ____________

name = 'matthews_corrcoef'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[mutual_info_score] ____________

name = 'mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[neg_brier_score] _____________

name = 'neg_brier_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[neg_log_loss] ______________

name = 'neg_log_loss'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____________ test_scorer_set_score_request_raises[neg_max_error] ______________

name = 'neg_max_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________ test_scorer_set_score_request_raises[neg_mean_absolute_error] _________

name = 'neg_mean_absolute_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___ test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error] ___

name = 'neg_mean_absolute_percentage_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________ test_scorer_set_score_request_raises[neg_mean_gamma_deviance] _________

name = 'neg_mean_gamma_deviance'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______ test_scorer_set_score_request_raises[neg_mean_poisson_deviance] ________

name = 'neg_mean_poisson_deviance'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_________ test_scorer_set_score_request_raises[neg_mean_squared_error] _________

name = 'neg_mean_squared_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______ test_scorer_set_score_request_raises[neg_mean_squared_log_error] _______

name = 'neg_mean_squared_log_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______ test_scorer_set_score_request_raises[neg_median_absolute_error] ________

name = 'neg_median_absolute_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____ test_scorer_set_score_request_raises[neg_negative_likelihood_ratio] ______

name = 'neg_negative_likelihood_ratio'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______ test_scorer_set_score_request_raises[neg_root_mean_squared_error] _______

name = 'neg_root_mean_squared_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____ test_scorer_set_score_request_raises[neg_root_mean_squared_log_error] _____

name = 'neg_root_mean_squared_log_error'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______ test_scorer_set_score_request_raises[normalized_mutual_info_score] ______

name = 'normalized_mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______ test_scorer_set_score_request_raises[positive_likelihood_ratio] ________

name = 'positive_likelihood_ratio'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______________ test_scorer_set_score_request_raises[precision] ________________

name = 'precision'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[precision_macro] _____________

name = 'precision_macro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[precision_micro] _____________

name = 'precision_micro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[precision_samples] ____________

name = 'precision_samples'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________ test_scorer_set_score_request_raises[precision_weighted] ___________

name = 'precision_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
___________________ test_scorer_set_score_request_raises[r2] ___________________

name = 'r2'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_______________ test_scorer_set_score_request_raises[rand_score] _______________

name = 'rand_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_________________ test_scorer_set_score_request_raises[recall] _________________

name = 'recall'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[recall_macro] ______________

name = 'recall_macro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[recall_micro] ______________

name = 'recall_micro'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____________ test_scorer_set_score_request_raises[recall_samples] _____________

name = 'recall_samples'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[recall_weighted] _____________

name = 'recall_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
________________ test_scorer_set_score_request_raises[roc_auc] _________________

name = 'roc_auc'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[roc_auc_ovo] _______________

name = 'roc_auc_ovo'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
__________ test_scorer_set_score_request_raises[roc_auc_ovo_weighted] __________

name = 'roc_auc_ovo_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
______________ test_scorer_set_score_request_raises[roc_auc_ovr] _______________

name = 'roc_auc_ovr'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
__________ test_scorer_set_score_request_raises[roc_auc_ovr_weighted] __________

name = 'roc_auc_ovr_weighted'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
_____________ test_scorer_set_score_request_raises[top_k_accuracy] _____________

name = 'top_k_accuracy'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________ test_scorer_set_score_request_raises[v_measure_score] _____________

name = 'v_measure_score'

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_set_score_request_raises(name):
        """Test that set_score_request is only available when feature flag is on."""
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
        with pytest.raises(RuntimeError, match="This method is only available"):
>           scorer.set_score_request()
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1215: AttributeError
____________________ test_scorer_metadata_request[accuracy] ____________________

name = 'accuracy'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________ test_scorer_metadata_request[adjusted_mutual_info_score] ___________

name = 'adjusted_mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
______________ test_scorer_metadata_request[adjusted_rand_score] _______________

name = 'adjusted_rand_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[average_precision] ________________

name = 'average_precision'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[balanced_accuracy] ________________

name = 'balanced_accuracy'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[completeness_score] _______________

name = 'completeness_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________ test_scorer_metadata_request[d2_absolute_error_score] _____________

name = 'd2_absolute_error_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[explained_variance] _______________

name = 'explained_variance'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________________ test_scorer_metadata_request[f1] _______________________

name = 'f1'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________________ test_scorer_metadata_request[f1_macro] ____________________

name = 'f1_macro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________________ test_scorer_metadata_request[f1_micro] ____________________

name = 'f1_micro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________________ test_scorer_metadata_request[f1_samples] ___________________

name = 'f1_samples'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[f1_weighted] ___________________

name = 'f1_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_____________ test_scorer_metadata_request[fowlkes_mallows_score] ______________

name = 'fowlkes_mallows_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[homogeneity_score] ________________

name = 'homogeneity_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________________ test_scorer_metadata_request[jaccard] _____________________

name = 'jaccard'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________ test_scorer_metadata_request[jaccard_macro] __________________

name = 'jaccard_macro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________ test_scorer_metadata_request[jaccard_micro] __________________

name = 'jaccard_micro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[jaccard_samples] _________________

name = 'jaccard_samples'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[jaccard_weighted] ________________

name = 'jaccard_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[matthews_corrcoef] ________________

name = 'matthews_corrcoef'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[mutual_info_score] ________________

name = 'mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[neg_brier_score] _________________

name = 'neg_brier_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[neg_log_loss] __________________

name = 'neg_log_loss'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________ test_scorer_metadata_request[neg_max_error] __________________

name = 'neg_max_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________ test_scorer_metadata_request[neg_mean_absolute_error] _____________

name = 'neg_mean_absolute_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______ test_scorer_metadata_request[neg_mean_absolute_percentage_error] _______

name = 'neg_mean_absolute_percentage_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________ test_scorer_metadata_request[neg_mean_gamma_deviance] _____________

name = 'neg_mean_gamma_deviance'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________ test_scorer_metadata_request[neg_mean_poisson_deviance] ____________

name = 'neg_mean_poisson_deviance'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_____________ test_scorer_metadata_request[neg_mean_squared_error] _____________

name = 'neg_mean_squared_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________ test_scorer_metadata_request[neg_mean_squared_log_error] ___________

name = 'neg_mean_squared_log_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________ test_scorer_metadata_request[neg_median_absolute_error] ____________

name = 'neg_median_absolute_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________ test_scorer_metadata_request[neg_negative_likelihood_ratio] __________

name = 'neg_negative_likelihood_ratio'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________ test_scorer_metadata_request[neg_root_mean_squared_error] ___________

name = 'neg_root_mean_squared_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________ test_scorer_metadata_request[neg_root_mean_squared_log_error] _________

name = 'neg_root_mean_squared_log_error'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________ test_scorer_metadata_request[normalized_mutual_info_score] __________

name = 'normalized_mutual_info_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________ test_scorer_metadata_request[positive_likelihood_ratio] ____________

name = 'positive_likelihood_ratio'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________________ test_scorer_metadata_request[precision] ____________________

name = 'precision'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[precision_macro] _________________

name = 'precision_macro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[precision_micro] _________________

name = 'precision_micro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[precision_samples] ________________

name = 'precision_samples'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________ test_scorer_metadata_request[precision_weighted] _______________

name = 'precision_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_______________________ test_scorer_metadata_request[r2] _______________________

name = 'r2'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
___________________ test_scorer_metadata_request[rand_score] ___________________

name = 'rand_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_____________________ test_scorer_metadata_request[recall] _____________________

name = 'recall'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[recall_macro] __________________

name = 'recall_macro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[recall_micro] __________________

name = 'recall_micro'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________ test_scorer_metadata_request[recall_samples] _________________

name = 'recall_samples'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[recall_weighted] _________________

name = 'recall_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
____________________ test_scorer_metadata_request[roc_auc] _____________________

name = 'roc_auc'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[roc_auc_ovo] ___________________

name = 'roc_auc_ovo'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
______________ test_scorer_metadata_request[roc_auc_ovo_weighted] ______________

name = 'roc_auc_ovo_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
__________________ test_scorer_metadata_request[roc_auc_ovr] ___________________

name = 'roc_auc_ovr'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
______________ test_scorer_metadata_request[roc_auc_ovr_weighted] ______________

name = 'roc_auc_ovr_weighted'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________ test_scorer_metadata_request[top_k_accuracy] _________________

name = 'top_k_accuracy'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
________________ test_scorer_metadata_request[v_measure_score] _________________

name = 'v_measure_score'

    @pytest.mark.parametrize("name", get_scorer_names(), ids=get_scorer_names())
    @config_context(enable_metadata_routing=True)
    def test_scorer_metadata_request(name):
        """Testing metadata requests for scorers.
    
        This test checks many small things in a large test, to reduce the
        boilerplate required for each section.
        """
        # Make sure they expose the routing methods.
        scorer = get_scorer(name)
>       assert hasattr(scorer, "set_score_request")
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1228: AssertionError
_________________________ test_metadata_kwarg_conflict _________________________

    @config_context(enable_metadata_routing=True)
    def test_metadata_kwarg_conflict():
        """This test makes sure the right warning is raised if the user passes
        some metadata both as a constructor to make_scorer, and during __call__.
        """
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
    
        scorer = make_scorer(
            roc_auc_score,
            response_method="predict_proba",
            multi_class="ovo",
            labels=lr.classes_,
        )
        with pytest.warns(UserWarning, match="already set as kwargs"):
>           scorer.set_score_request(labels=True)
E           AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1286: AttributeError

During handling of the above exception, another exception occurred:

    @config_context(enable_metadata_routing=True)
    def test_metadata_kwarg_conflict():
        """This test makes sure the right warning is raised if the user passes
        some metadata both as a constructor to make_scorer, and during __call__.
        """
        X, y = make_classification(
            n_classes=3, n_informative=3, n_samples=20, random_state=0
        )
        lr = LogisticRegression().fit(X, y)
    
        scorer = make_scorer(
            roc_auc_score,
            response_method="predict_proba",
            multi_class="ovo",
            labels=lr.classes_,
        )
        with pytest.warns(UserWarning, match="already set as kwargs"):
>           scorer.set_score_request(labels=True)
E           Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.
E            Emitted warnings: [].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1286: Failed
__________________ test_multimetric_scoring_metadata_routing ___________________

    @config_context(enable_metadata_routing=True)
    def test_multimetric_scoring_metadata_routing():
        # Test that _MultimetricScorer properly routes metadata.
        def score1(y_true, y_pred):
            return 1
    
        def score2(y_true, y_pred, sample_weight="test"):
            # make sure sample_weight is not passed
            assert sample_weight == "test"
            return 1
    
        def score3(y_true, y_pred, sample_weight=None):
            # make sure sample_weight is passed
            assert sample_weight is not None
            return 1
    
        scorers = {
            "score1": make_scorer(score1),
>           "score2": make_scorer(score2).set_score_request(sample_weight=False),
            "score3": make_scorer(score3).set_score_request(sample_weight=True),
        }
E       AttributeError: 'Scorer' object has no attribute 'set_score_request'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1345: AttributeError
__________________ test_kwargs_without_metadata_routing_error __________________

    def test_kwargs_without_metadata_routing_error():
        # Test that kwargs are not supported in scorers if metadata routing is not
        # enabled.
        # TODO: remove when enable_metadata_routing is deprecated
        def score(y_true, y_pred, param=None):
            return 1  # pragma: no cover
    
        X, y = make_classification(
            n_samples=50, n_features=2, n_redundant=0, random_state=0
        )
    
        clf = DecisionTreeClassifier().fit(X, y)
        scorer = make_scorer(score)
        with config_context(enable_metadata_routing=False):
            with pytest.raises(
                ValueError, match="is only supported if enable_metadata_routing=True"
            ):
>               scorer(clf, X, y, param="blah")
E               TypeError: __call__() got an unexpected keyword argument 'param'

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1385: TypeError
_____________________ test_get_scorer_multilabel_indicator _____________________

    def test_get_scorer_multilabel_indicator():
        """Check that our scorer deal with multi-label indicator matrices.
    
        Non-regression test for:
        https://github.com/scikit-learn/scikit-learn/issues/26817
        """
        X, Y = make_multilabel_classification(n_samples=72, n_classes=3, random_state=0)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)
    
        estimator = KNeighborsClassifier().fit(X_train, Y_train)
    
        score = get_scorer("average_precision")(estimator, X_test, Y_test)
>       assert score > 0.8
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1400: AssertionError
_ test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')] _

scorer = <sklearn.metrics.temp.Scorer object at 0x70c891653520>
expected_repr = "make_scorer(accuracy_score, response_method='predict')"

    @pytest.mark.parametrize(
        "scorer, expected_repr",
        [
            (
                get_scorer("accuracy"),
                "make_scorer(accuracy_score, response_method='predict')",
            ),
            (
                get_scorer("neg_log_loss"),
                (
                    "make_scorer(log_loss, greater_is_better=False,"
                    " response_method='predict_proba')"
                ),
            ),
            (
                get_scorer("roc_auc"),
                (
                    "make_scorer(roc_auc_score, response_method="
                    "('decision_function', 'predict_proba'))"
                ),
            ),
            (
                make_scorer(fbeta_score, beta=2),
                "make_scorer(fbeta_score, response_method='predict', beta=2)",
            ),
        ],
    )
    def test_make_scorer_repr(scorer, expected_repr):
        """Check the representation of the scorer."""
>       assert repr(scorer) == expected_repr
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1432: AssertionError
_ test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')] _

scorer = <sklearn.metrics.temp.Scorer object at 0x70c891653880>
expected_repr = "make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')"

    @pytest.mark.parametrize(
        "scorer, expected_repr",
        [
            (
                get_scorer("accuracy"),
                "make_scorer(accuracy_score, response_method='predict')",
            ),
            (
                get_scorer("neg_log_loss"),
                (
                    "make_scorer(log_loss, greater_is_better=False,"
                    " response_method='predict_proba')"
                ),
            ),
            (
                get_scorer("roc_auc"),
                (
                    "make_scorer(roc_auc_score, response_method="
                    "('decision_function', 'predict_proba'))"
                ),
            ),
            (
                make_scorer(fbeta_score, beta=2),
                "make_scorer(fbeta_score, response_method='predict', beta=2)",
            ),
        ],
    )
    def test_make_scorer_repr(scorer, expected_repr):
        """Check the representation of the scorer."""
>       assert repr(scorer) == expected_repr
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1432: AssertionError
_ test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))] _

scorer = <sklearn.metrics.temp.Scorer object at 0x70c891653ca0>
expected_repr = "make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))"

    @pytest.mark.parametrize(
        "scorer, expected_repr",
        [
            (
                get_scorer("accuracy"),
                "make_scorer(accuracy_score, response_method='predict')",
            ),
            (
                get_scorer("neg_log_loss"),
                (
                    "make_scorer(log_loss, greater_is_better=False,"
                    " response_method='predict_proba')"
                ),
            ),
            (
                get_scorer("roc_auc"),
                (
                    "make_scorer(roc_auc_score, response_method="
                    "('decision_function', 'predict_proba'))"
                ),
            ),
            (
                make_scorer(fbeta_score, beta=2),
                "make_scorer(fbeta_score, response_method='predict', beta=2)",
            ),
        ],
    )
    def test_make_scorer_repr(scorer, expected_repr):
        """Check the representation of the scorer."""
>       assert repr(scorer) == expected_repr
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1432: AssertionError
_ test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)] _

scorer = <sklearn.metrics.temp.Scorer object at 0x70c891653e80>
expected_repr = "make_scorer(fbeta_score, response_method='predict', beta=2)"

    @pytest.mark.parametrize(
        "scorer, expected_repr",
        [
            (
                get_scorer("accuracy"),
                "make_scorer(accuracy_score, response_method='predict')",
            ),
            (
                get_scorer("neg_log_loss"),
                (
                    "make_scorer(log_loss, greater_is_better=False,"
                    " response_method='predict_proba')"
                ),
            ),
            (
                get_scorer("roc_auc"),
                (
                    "make_scorer(roc_auc_score, response_method="
                    "('decision_function', 'predict_proba'))"
                ),
            ),
            (
                make_scorer(fbeta_score, beta=2),
                "make_scorer(fbeta_score, response_method='predict', beta=2)",
            ),
        ],
    )
    def test_make_scorer_repr(scorer, expected_repr):
        """Check the representation of the scorer."""
>       assert repr(scorer) == expected_repr
E       AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1432: AssertionError
______________________ test_get_scorer_multimetric[True] _______________________

pass_estimator = True

    @pytest.mark.parametrize("pass_estimator", [True, False])
    def test_get_scorer_multimetric(pass_estimator):
        """Check that check_scoring is compatible with multi-metric configurations."""
        X, y = make_classification(n_samples=150, n_features=10, random_state=0)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LogisticRegression(random_state=0)
    
        if pass_estimator:
            check_scoring_ = check_scoring
        else:
            check_scoring_ = partial(check_scoring, clf)
    
        clf.fit(X_train, y_train)
    
        y_pred = clf.predict(X_test)
        y_proba = clf.predict_proba(X_test)
    
        expected_results = {
            "r2": r2_score(y_test, y_pred),
            "roc_auc": roc_auc_score(y_test, y_proba[:, 1]),
            "accuracy": accuracy_score(y_test, y_pred),
        }
    
        for container in [set, list, tuple]:
            scoring = check_scoring_(scoring=container(["r2", "roc_auc", "accuracy"]))
            result = scoring(clf, X_test, y_test)
    
            assert result.keys() == expected_results.keys()
            for name in result:
>               assert result[name] == pytest.approx(expected_results[name])
E               AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1464: AssertionError
______________________ test_get_scorer_multimetric[False] ______________________

pass_estimator = False

    @pytest.mark.parametrize("pass_estimator", [True, False])
    def test_get_scorer_multimetric(pass_estimator):
        """Check that check_scoring is compatible with multi-metric configurations."""
        X, y = make_classification(n_samples=150, n_features=10, random_state=0)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LogisticRegression(random_state=0)
    
        if pass_estimator:
            check_scoring_ = check_scoring
        else:
            check_scoring_ = partial(check_scoring, clf)
    
        clf.fit(X_train, y_train)
    
        y_pred = clf.predict(X_test)
        y_proba = clf.predict_proba(X_test)
    
        expected_results = {
            "r2": r2_score(y_test, y_pred),
            "roc_auc": roc_auc_score(y_test, y_proba[:, 1]),
            "accuracy": accuracy_score(y_test, y_pred),
        }
    
        for container in [set, list, tuple]:
            scoring = check_scoring_(scoring=container(["r2", "roc_auc", "accuracy"]))
            result = scoring(clf, X_test, y_test)
    
            assert result.keys() == expected_results.keys()
            for name in result:
>               assert result[name] == pytest.approx(expected_results[name])
E               AssertionError

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1464: AssertionError
___________________ test_check_scoring_multimetric_raise_exc ___________________

    def test_check_scoring_multimetric_raise_exc():
        """Test that check_scoring returns error code for a subset of scorers in
        multimetric scoring if raise_exc=False and raises otherwise."""
    
        def raising_scorer(estimator, X, y):
            raise ValueError("That doesn't work.")
    
        X, y = make_classification(n_samples=150, n_features=10, random_state=0)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        clf = LogisticRegression().fit(X_train, y_train)
    
        # "raising_scorer" is raising ValueError and should return an string representation
        # of the error of the last scorer:
        scoring = {
            "accuracy": make_scorer(accuracy_score),
            "raising_scorer": raising_scorer,
        }
>       scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=False)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1512: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:675: in check_scoring
    scorers = _check_multimetric_scoring(estimator, scoring=scoring)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:430: in _check_multimetric_scoring
    scorers = {key: check_scoring(estimator, scoring=scorer) for key, scorer in scoring.items()}
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:430: in <dictcomp>
    scorers = {key: check_scoring(estimator, scoring=scorer) for key, scorer in scoring.items()}
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
scoring = <sklearn.metrics.temp.Scorer object at 0x70c87d2a6a30>

    @validate_params({'estimator': [HasMethods('fit'), None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, set, tuple, dict, None], 'allow_none': ['boolean'], 'raise_exc': ['boolean']}, prefer_skip_nested_validation=True)
    def check_scoring(estimator=None, scoring=None, *, allow_none=False, raise_exc=True):
        """Determine scorer from user options.
    
        A TypeError will be thrown if the estimator cannot be scored.
    
        Parameters
        ----------
        estimator : estimator object implementing 'fit' or None, default=None
            The object to use to fit the data. If `None`, then this function may error
            depending on `allow_none`.
    
        scoring : str, callable, list, tuple, set, or dict, default=None
            Scorer to use. If `scoring` represents a single score, one can use:
    
            - a single string (see :ref:`scoring_parameter`);
            - a callable (see :ref:`scoring`) that returns a single value.
    
            If `scoring` represents multiple scores, one can use:
    
            - a list, tuple or set of unique strings;
            - a callable returning a dictionary where the keys are the metric names and the
              values are the metric scorers;
            - a dictionary with metric names as keys and callables a values. The callables
              need to have the signature `callable(estimator, X, y)`.
    
            If None, the provided estimator object's `score` method is used.
    
        allow_none : bool, default=False
            Whether to return None or raise an error if no `scoring` is specified and the
            estimator has no `score` method.
    
        raise_exc : bool, default=True
            Whether to raise an exception (if a subset of the scorers in multimetric scoring
            fails) or to return an error code.
    
            - If set to `True`, raises the failing scorer's exception.
            - If set to `False`, a formatted string of the exception details is passed as
              result of the failing scorer(s).
    
            This applies if `scoring` is list, tuple, set, or dict. Ignored if `scoring` is
            a str or a callable.
    
            .. versionadded:: 1.6
    
        Returns
        -------
        scoring : callable
            A scorer callable object / function with signature ``scorer(estimator, X, y)``.
    
        Examples
        --------
        >>> from sklearn.datasets import load_iris
        >>> from sklearn.metrics import check_scoring
        >>> from sklearn.tree import DecisionTreeClassifier
        >>> X, y = load_iris(return_X_y=True)
        >>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)
        >>> scorer = check_scoring(classifier, scoring='accuracy')
        >>> scorer(classifier, X, y)
        0.96...
    
        >>> from sklearn.metrics import make_scorer, accuracy_score, mean_squared_log_error
        >>> X, y = load_iris(return_X_y=True)
        >>> y *= -1
        >>> clf = DecisionTreeClassifier().fit(X, y)
        >>> scoring = {
        ...     "accuracy": make_scorer(accuracy_score),
        ...     "mean_squared_log_error": make_scorer(mean_squared_log_error),
        ... }
        >>> scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=False)
        >>> scores = scoring_call(clf, X, y)
        >>> scores
        {'accuracy': 1.0, 'mean_squared_log_error': 'Traceback ...'}
        """
        if isinstance(scoring, str):
            return get_scorer(scoring)
        if callable(scoring):
            module = getattr(scoring, '__module__', None)
            if hasattr(module, 'startswith') and module.startswith('sklearn.metrics.') and (not module.startswith('sklearn.metrics._scorer')) and (not module.startswith('sklearn.metrics.tests.')):
>               raise ValueError('scoring value %r looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.' % scoring)
E               ValueError: scoring value <sklearn.metrics.temp.Scorer object at 0x70c87d2a6a30> looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:672: ValueError
___________ test_metadata_routing_multimetric_metadata_routing[True] ___________

enable_metadata_routing = True

    @pytest.mark.parametrize("enable_metadata_routing", [True, False])
    def test_metadata_routing_multimetric_metadata_routing(enable_metadata_routing):
        """Test multimetric scorer works with and without metadata routing enabled when
        there is no actual metadata to pass.
    
        Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/28256
        """
        X, y = make_classification(n_samples=50, n_features=10, random_state=0)
        estimator = EstimatorWithFitAndPredict().fit(X, y)
    
        multimetric_scorer = _MultimetricScorer(scorers={"acc": get_scorer("accuracy")})
        with config_context(enable_metadata_routing=enable_metadata_routing):
>           multimetric_scorer(estimator, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:82: in __call__
    raise e
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:78: in __call__
    score = scorer(estimator, *args, **routed_params.get(name).score)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
__________ test_metadata_routing_multimetric_metadata_routing[False] ___________

enable_metadata_routing = False

    @pytest.mark.parametrize("enable_metadata_routing", [True, False])
    def test_metadata_routing_multimetric_metadata_routing(enable_metadata_routing):
        """Test multimetric scorer works with and without metadata routing enabled when
        there is no actual metadata to pass.
    
        Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/28256
        """
        X, y = make_classification(n_samples=50, n_features=10, random_state=0)
        estimator = EstimatorWithFitAndPredict().fit(X, y)
    
        multimetric_scorer = _MultimetricScorer(scorers={"acc": get_scorer("accuracy")})
        with config_context(enable_metadata_routing=enable_metadata_routing):
>           multimetric_scorer(estimator, X, y)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:82: in __call__
    raise e
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:78: in __call__
    score = scorer(estimator, *args, **routed_params.get(name).score)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/temp.py:31: in __call__
    check_is_fitted(estimator)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = EstimatorWithFitAndPredict(), attributes = None

    def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
        """Perform is_fitted validation for estimator.
    
        Checks if the estimator is fitted by verifying the presence of
        fitted attributes (ending with a trailing underscore) and otherwise
        raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.
    
        If an estimator does not set any attributes with a trailing underscore, it
        can define a ``__sklearn_is_fitted__`` method returning a boolean to
        specify if the estimator is fitted or not. See
        :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`
        for an example on how to use the API.
    
        If no `attributes` are passed, this fuction will pass if an estimator is stateless.
        An estimator can indicate it's stateless by setting the `requires_fit` tag. See
        :ref:`estimator_tags` for more information. Note that the `requires_fit` tag
        is ignored if `attributes` are passed.
    
        Parameters
        ----------
        estimator : estimator instance
            Estimator instance for which the check is performed.
    
        attributes : str, list or tuple of str, default=None
            Attribute name(s) given as string or a list/tuple of strings
            Eg.: ``["coef_", "estimator_", ...], "coef_"``
    
            If `None`, `estimator` is considered fitted if there exist an
            attribute that ends with a underscore and does not start with double
            underscore.
    
        msg : str, default=None
            The default error message is, "This %(name)s instance is not fitted
            yet. Call 'fit' with appropriate arguments before using this
            estimator."
    
            For custom messages if "%(name)s" is present in the message string,
            it is substituted for the estimator name.
    
            Eg. : "Estimator, %(name)s, must be fitted before sparsifying".
    
        all_or_any : callable, {all, any}, default=all
            Specify whether all or any of the given attributes must exist.
    
        Raises
        ------
        TypeError
            If the estimator is a class or not an estimator instance
    
        NotFittedError
            If the attributes are not found.
    
        Examples
        --------
        >>> from sklearn.linear_model import LogisticRegression
        >>> from sklearn.utils.validation import check_is_fitted
        >>> from sklearn.exceptions import NotFittedError
        >>> lr = LogisticRegression()
        >>> try:
        ...     check_is_fitted(lr)
        ... except NotFittedError as exc:
        ...     print(f"Model is not fitted yet.")
        Model is not fitted yet.
        >>> lr.fit([[1, 2], [1, 3]], [1, 0])
        LogisticRegression()
        >>> check_is_fitted(lr)
        """
        if isclass(estimator):
            raise TypeError("{} is a class, not an instance.".format(estimator))
        if msg is None:
            msg = (
                "This %(name)s instance is not fitted yet. Call 'fit' with "
                "appropriate arguments before using this estimator."
            )
    
        if not hasattr(estimator, "fit"):
            raise TypeError("%s is not an estimator instance." % (estimator))
    
        tags = get_tags(estimator)
    
        if not tags.requires_fit and attributes is None:
            return
    
        if not _is_fitted(estimator, attributes, all_or_any):
>           raise NotFittedError(msg % {"name": type(estimator).__name__})
E           sklearn.exceptions.NotFittedError: This EstimatorWithFitAndPredict instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

../publishablew/scikit-learn/scikit-learn/sklearn/utils/validation.py:1746: NotFittedError
_______________ test_make_scorer_reponse_method_default_warning ________________

    def test_make_scorer_reponse_method_default_warning():
        with pytest.warns(FutureWarning, match="response_method=None is deprecated"):
>           make_scorer(accuracy_score, response_method=None)
E           Failed: DID NOT WARN. No warnings of type (<class 'FutureWarning'>,) were emitted.
E            Emitted warnings: [].

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:1632: Failed
=========================== short test summary info ============================
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning
================= 187 failed, 86 passed, 13 warnings in 5.28s ==================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 273 items

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_all_scorers_repr I: Seeding RNGs with 1473666748
PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of callables] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[list of int] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of one callable] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-unique str] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-string key dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1-f1_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_weighted-metric1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_macro-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_micro-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision-precision_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall-recall_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard-jaccard_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_weighted-metric13] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_macro-metric14] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_micro-metric15] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[top_k_accuracy-top_k_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[matthews_corrcoef-matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[accuracy-accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[balanced_accuracy-balanced_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorers PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scoring_is_not_metric PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_sanity_check PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_return_copy PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_no_op_multiclass_select_proba PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request_raises_without_routing_enabled PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_repr PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer_pos_label[42] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning PASSED

=================================== FAILURES ===================================
_______________________ test_supervised_cluster_scorers ________________________

    def test_supervised_cluster_scorers():
        # Test clustering scorers against gold standard labeling.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        km = KMeans(n_clusters=3, n_init="auto")
        km.fit(X_train)
        for name in CLUSTER_SCORERS:
>           score1 = get_scorer(name)(km, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:550: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 1, 1])
labels_pred = array([1, 2, 0, 0, 0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 2, 1, 2, 1, 1, 0, 0, 0,
       1, 1, 2], dtype=int32)

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
________________ test_scorer_memmap_input[adjusted_rand_score] _________________

name = 'adjusted_rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
_____________________ test_scorer_memmap_input[rand_score] _____________________

name = 'rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:259: in rand_score
    contingency = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
=========================== short test summary info ============================
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score]
================== 3 failed, 270 passed, 16 warnings in 1.30s ==================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 273 items

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_all_scorers_repr I: Seeding RNGs with 301577216
PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[single_list] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_str] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[multi_list] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring[dict_callable] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of callables] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[list of int] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[tuple of one callable] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty tuple] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-unique str] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[non-string key dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring_errors[empty dict] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1-f1_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_weighted-metric1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_macro-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[f1_micro-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision-precision_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_weighted-metric5] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_macro-metric6] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[precision_micro-metric7] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall-recall_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_weighted-metric9] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_macro-metric10] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[recall_micro-metric11] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard-jaccard_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_weighted-metric13] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_macro-metric14] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[jaccard_micro-metric15] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[top_k_accuracy-top_k_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_binary_scores[matthews_corrcoef-matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[accuracy-accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[balanced_accuracy-balanced_accuracy_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_weighted-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_macro-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[f1_micro-metric4] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_weighted-metric5] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_macro-metric6] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[precision_micro-metric7] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_weighted-metric8] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_macro-metric9] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[recall_micro-metric10] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_weighted-metric11] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_macro-metric12] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_multiclass_scores[jaccard_micro-metric13] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_custom_scorer_pickling PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorers PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_classification_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_regression_scorer_sample_weight PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score] FAILED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scoring_is_not_metric PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision[scorers1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_sanity_check PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_exception_handling[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr-metric0] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo-metric1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovr_weighted-metric2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer[roc_auc_ovo_weighted-metric3] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_proba_scorer_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multiclass_roc_no_proba_scorer_errors[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_average_precision_pos_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_brier_score_loss_pos_label PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[f1_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[precision_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[recall_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_non_symmetric_metric_pos_label[jaccard_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[non-thresholded scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[probability scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_select_proba_error[thresholded scorer] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_return_copy PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_no_op_multiclass_select_proba PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[adjusted_rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_set_score_request_raises[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[adjusted_rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[average_precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[balanced_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[completeness_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[d2_absolute_error_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[explained_variance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[f1_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[fowlkes_mallows_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[homogeneity_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[jaccard_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[matthews_corrcoef] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_brier_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_log_loss] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_max_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_absolute_percentage_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_gamma_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_poisson_deviance] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_median_absolute_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_negative_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[neg_root_mean_squared_log_error] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[normalized_mutual_info_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[positive_likelihood_ratio] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[precision_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[r2] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[rand_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_macro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_micro] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_samples] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[recall_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovo_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[roc_auc_ovr_weighted] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[top_k_accuracy] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_metadata_request[v_measure_score] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_kwarg_conflict PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_PassthroughScorer_set_score_request_raises_without_routing_enabled PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scoring_metadata_routing PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_kwargs_without_metadata_routing_error PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multilabel_indicator PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer0-make_scorer(accuracy_score, response_method='predict')] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer1-make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer2-make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'))] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_repr[scorer3-make_scorer(fbeta_score, response_method='predict', beta=2)] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_get_scorer_multimetric[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_repr PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_check_scoring_multimetric_raise_exc PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[True] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_metadata_routing_multimetric_metadata_routing[False] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_curve_scorer_pos_label[42] PASSED
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_make_scorer_reponse_method_default_warning PASSED

=================================== FAILURES ===================================
_______________________ test_supervised_cluster_scorers ________________________

    def test_supervised_cluster_scorers():
        # Test clustering scorers against gold standard labeling.
        X, y = make_blobs(random_state=0, centers=2)
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        km = KMeans(n_clusters=3, n_init="auto")
        km.fit(X_train)
        for name in CLUSTER_SCORERS:
>           score1 = get_scorer(name)(km, X_test, y_test)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:550: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 1, 1])
labels_pred = array([0, 0, 1, 1, 1, 0, 0, 1, 2, 1, 1, 2, 1, 1, 0, 0, 2, 0, 0, 1, 2, 1,
       2, 0, 0], dtype=int32)

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
________________ test_scorer_memmap_input[adjusted_rand_score] _________________

name = 'adjusted_rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:361: in adjusted_rand_score
    ((tn, fp), (fn, tp)) = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
_____________________ test_scorer_memmap_input[rand_score] _____________________

name = 'rand_score'
memmap_data_and_estimators = (memmap([[ 0.84726844,  1.05445173, -1.38667072, -0.58147661,  1.52127328],
        [ 0.60950495,  0.70657317, -1.0846...rand_score': DecisionTreeClassifier(random_state=0), 'average_precision': DecisionTreeClassifier(random_state=0), ...})

    @pytest.mark.parametrize("name", get_scorer_names())
    def test_scorer_memmap_input(name, memmap_data_and_estimators):
        # Non-regression test for #6147: some score functions would
        # return singleton memmap when computed on memmap data instead of scalar
        # float values.
        X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
    
        if name in REQUIRE_POSITIVE_Y_SCORERS:
            y_mm_1 = _require_positive_y(y_mm)
            y_ml_mm_1 = _require_positive_y(y_ml_mm)
        else:
            y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
    
        # UndefinedMetricWarning for P / R scores
        with ignore_warnings():
            scorer, estimator = get_scorer(name), estimators[name]
            if name in MULTILABEL_ONLY_SCORERS:
                score = scorer(estimator, X_mm, y_ml_mm_1)
            else:
>               score = scorer(estimator, X_mm, y_mm_1)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py:693: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:288: in __call__
    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/_scorer.py:388: in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:259: in rand_score
    contingency = pair_confusion_matrix(labels_true, labels_pred)
../publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:189: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

labels_true = memmap([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1])
labels_pred = array([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 0, 1])

    @validate_params({'labels_true': ['array-like'], 'labels_pred': ['array-like']}, prefer_skip_nested_validation=True)
    def pair_confusion_matrix(labels_true, labels_pred):
>       from .temp import pair_confusion_matrix
E       ImportError: cannot import name 'pair_confusion_matrix' from 'sklearn.metrics.cluster.temp' (/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/temp.py)

../publishablew/scikit-learn/scikit-learn/sklearn/metrics/cluster/_supervised.py:121: ImportError
=========================== short test summary info ============================
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score]
FAILED ../publishablew/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[rand_score]
================== 3 failed, 270 passed, 16 warnings in 1.67s ==================
